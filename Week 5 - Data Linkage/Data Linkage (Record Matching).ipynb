{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\r\n",
    "import recordlinkage\r\n",
    "import time\r\n",
    "# Get the true matches for this linkage dataset \r\n",
    "# See: https://recordlinkage.readthedocs.io/en/latest/notebooks/link_two_dataframes.html)\r\n",
    "from recordlinkage.datasets import load_febrl1\r\n",
    "import re\r\n",
    "import mmh3\r\n",
    "from nltk.util import bigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task\r\n",
    "Perform data linkage to identify these matches between the 2 datasets.\r\n",
    "\r\n",
    "## Datasets\r\n",
    "The datasets used in this worksheet is obtained from the `recordlinkage` library. It contains 2 dataframes of size 500, consisting of 8 personal detail fields. The record with index `rec-0-org` from dataset 1 has a match with the record with index `rec-0-dup-0` from dataset 2; the record with index `rec-1-org` from dataset 1 has a match with the record with index `rec-1-dup-0` from dataset 2; etc.\r\n",
    "\r\n",
    "## Steps\r\n",
    "- Data linkage steps\r\n",
    "- Linkage algorithm performance evaluation\r\n",
    "- Blocking implementation\r\n",
    "- Bloom filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>given_name</th>\n      <th>surname</th>\n      <th>street_number</th>\n      <th>address_1</th>\n      <th>suburb</th>\n      <th>postcode</th>\n      <th>state</th>\n      <th>date_of_birth</th>\n    </tr>\n    <tr>\n      <th>rec_id</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>rec-0-org</th>\n      <td>flynn</td>\n      <td>rokobaro</td>\n      <td>12</td>\n      <td>herschell circuit</td>\n      <td>lawrence</td>\n      <td>2227</td>\n      <td>nsw</td>\n      <td>19720812</td>\n    </tr>\n    <tr>\n      <th>rec-1-org</th>\n      <td>karli</td>\n      <td>alderson</td>\n      <td>144</td>\n      <td>nulsen circuit</td>\n      <td>tingalpa</td>\n      <td>3139</td>\n      <td>nsw</td>\n      <td>19510826</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
      "text/plain": "          given_name   surname  street_number          address_1    suburb  \\\nrec_id                                                                       \nrec-0-org      flynn  rokobaro             12  herschell circuit  lawrence   \nrec-1-org      karli  alderson            144     nulsen circuit  tingalpa   \n\n          postcode state date_of_birth  \nrec_id                                  \nrec-0-org     2227   nsw      19720812  \nrec-1-org     3139   nsw      19510826  "
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>given_name</th>\n      <th>surname</th>\n      <th>street_number</th>\n      <th>address_1</th>\n      <th>suburb</th>\n      <th>postcode</th>\n      <th>state</th>\n      <th>date_of_birth</th>\n    </tr>\n    <tr>\n      <th>rec_id</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>rec-0-dup-0</th>\n      <td>thomas</td>\n      <td>rokobaro</td>\n      <td>12</td>\n      <td>herschell circuit</td>\n      <td>lawrence</td>\n      <td>2272</td>\n      <td>nsw</td>\n      <td>nan</td>\n    </tr>\n    <tr>\n      <th>rec-1-dup-0</th>\n      <td>karli</td>\n      <td>alderson</td>\n      <td>144</td>\n      <td>nulsen circuit</td>\n      <td>tings lpa</td>\n      <td>3139</td>\n      <td>nsw</td>\n      <td>19510826.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
      "text/plain": "            given_name   surname  street_number          address_1     suburb  \\\nrec_id                                                                          \nrec-0-dup-0     thomas  rokobaro             12  herschell circuit   lawrence   \nrec-1-dup-0      karli  alderson            144     nulsen circuit  tings lpa   \n\n            postcode state date_of_birth  \nrec_id                                    \nrec-0-dup-0     2272   nsw           nan  \nrec-1-dup-0     3139   nsw    19510826.0  "
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load toy datasets\n",
    "sample1 = pd.read_csv('datalinkage_sample1.csv').set_index('rec_id')\n",
    "sample2 = pd.read_csv('datalinkage_sample2.csv').set_index('rec_id')\n",
    "sample1['date_of_birth'] = sample1['date_of_birth'].astype(str)\n",
    "sample2['date_of_birth'] = sample2['date_of_birth'].astype(str)\n",
    "sample1['postcode'] = sample1['postcode'].astype(str)\n",
    "sample2['postcode'] = sample2['postcode'].astype(str)\n",
    "display(sample1)\n",
    "display(sample2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Linkage Pipeline\r\n",
    "\r\n",
    "A data linkage / record matching pipeline between dataset A and B usually follows 4 steps:\r\n",
    "\r\n",
    "1. Pre-process A and B & determine the features/attributes to calculate the similarity score\r\n",
    "2. Generate a list of (indexA, indexB) to compare against one another. The strategy to generate the list can either be exhaustive (every record in A against every record in B); random; or via blocking.\r\n",
    "3. For each (indexA, indexB), calculate the similarity score\r\n",
    "4. Choose a threshold. If the score is >= this threshold, then indexA and indexB is a match.\r\n",
    "\r\n",
    "For this tutorial, both datasets have been pre-processed, so we can skip step 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Generate the index pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:recordlinkage:indexing - performance warning - A full index can result in large number of record pairs.\n",
      "MultiIndex([('rec-0-org', 'rec-0-dup-0'),\n",
      "            ('rec-0-org', 'rec-1-dup-0'),\n",
      "            ('rec-1-org', 'rec-0-dup-0'),\n",
      "            ('rec-1-org', 'rec-1-dup-0')],\n",
      "           names=['rec_id_1', 'rec_id_2'])\n",
      "Number of pairs to perform matching: 4\n"
     ]
    }
   ],
   "source": [
    "def get_exhaustive_pairs(dfA, dfB):\n",
    "    \n",
    "    indexer = recordlinkage.Index()\n",
    "    indexer.full()\n",
    "    return indexer.index(dfA, dfB)\n",
    "\n",
    "exhaustive_sample = get_exhaustive_pairs(sample1, sample2)\n",
    "print(exhaustive_sample)\n",
    "print('Number of pairs to perform matching:', len(exhaustive_sample))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Calculate the similarity score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_sim(index_pairs, dfA, dfB):\n",
    "    \n",
    "    \"\"\"\n",
    "    Perform linkage based on the index_pairs generated in step 2.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Instantiate the scorer\n",
    "    scorer = recordlinkage.Compare()\n",
    "    \n",
    "    # Define the scoring function for each feature. See https://recordlinkage.readthedocs.io/en/latest/ref-compare.html#module-recordlinkage.compare\n",
    "    scorer.string('given_name', 'given_name', method='levenshtein', threshold=0.85, label='given_name')\n",
    "    scorer.string('surname', 'surname', method='levenshtein', threshold=0.85, label='surname')\n",
    "    scorer.string('date_of_birth', 'date_of_birth',  method='levenshtein', threshold=0.75, label='date_of_birth')\n",
    "    scorer.exact('suburb', 'suburb', label='suburb')\n",
    "    scorer.exact('state', 'state', label='state')\n",
    "    scorer.string('postcode', 'postcode', label='postcode')\n",
    "    scorer.string('address_1', 'address_1', method='levenshtein', threshold=0.85, label='address_1')\n",
    "    \n",
    "    # Perform the scoring and get the runtime for performance evaluation\n",
    "    start = time.time()\n",
    "    pairwise_sim = scorer.compute(index_pairs, dfA, dfB)\n",
    "    end = time.time()\n",
    "    runtime = end- start\n",
    "    \n",
    "    return pairwise_sim, runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matching time (secs): 0.012005805969238281\n"
     ]
    }
   ],
   "source": [
    "pairwise_sim, run_time = score_sim(exhaustive_sample, sample1, sample2)\n",
    "print('Matching time (secs):', run_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Compare against threshold and determining the matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th></th>\n      <th>given_name</th>\n      <th>surname</th>\n      <th>date_of_birth</th>\n      <th>suburb</th>\n      <th>state</th>\n      <th>postcode</th>\n      <th>address_1</th>\n    </tr>\n    <tr>\n      <th>rec_id_1</th>\n      <th>rec_id_2</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th rowspan=\"2\" valign=\"top\">rec-0-org</th>\n      <th>rec-0-dup-0</th>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0.5</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>rec-1-dup-0</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th rowspan=\"2\" valign=\"top\">rec-1-org</th>\n      <th>rec-0-dup-0</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>rec-1-dup-0</th>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1.0</td>\n      <td>1.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
      "text/plain": "                       given_name  surname  date_of_birth  suburb  state  \\\nrec_id_1  rec_id_2                                                         \nrec-0-org rec-0-dup-0         0.0      1.0            0.0       1      1   \n          rec-1-dup-0         0.0      0.0            0.0       0      1   \nrec-1-org rec-0-dup-0         0.0      0.0            0.0       0      1   \n          rec-1-dup-0         1.0      1.0            1.0       0      1   \n\n                       postcode  address_1  \nrec_id_1  rec_id_2                          \nrec-0-org rec-0-dup-0       0.5        1.0  \n          rec-1-dup-0       0.0        0.0  \nrec-1-org rec-0-dup-0       0.0        0.0  \n          rec-1-dup-0       1.0        1.0  "
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Investigate the output\n",
    "pairwise_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "rec_id_1   rec_id_2   \nrec-1-org  rec-1-dup-0    6.0\nrec-0-org  rec-0-dup-0    4.5\n           rec-1-dup-0    1.0\nrec-1-org  rec-0-dup-0    1.0\ndtype: float64"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Row sum and sort\n",
    "pairwise_sim.sum(axis=1).sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "MultiIndex([('rec-0-org', 'rec-0-dup-0'),\n            ('rec-1-org', 'rec-1-dup-0')],\n           names=['rec_id_1', 'rec_id_2'])"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_match(pairwise_sim, min_score=4):\n",
    "    scores = pairwise_sim.sum(axis=1)\n",
    "    return scores[scores>=min_score].index\n",
    "\n",
    "matched_results = get_match(pairwise_sim)\n",
    "matched_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform on full dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset1 = pd.read_csv('datalinkage_linkage1.csv').set_index('rec_id') # 500 rows\n",
    "dataset2 = pd.read_csv('datalinkage_linkage2.csv').set_index('rec_id') # 500 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset1['date_of_birth'] = dataset1['date_of_birth'].astype(str)\n",
    "dataset2['date_of_birth'] = dataset2['date_of_birth'].astype(str)\n",
    "dataset1['postcode'] = dataset1['postcode'].astype(str)\n",
    "dataset2['postcode'] = dataset2['postcode'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:recordlinkage:indexing - performance warning - A full index can result in large number of record pairs.\n",
      "Number of pairs to perform matching: 250000\n",
      "Matching time (secs): 9.238903284072876\n"
     ]
    }
   ],
   "source": [
    "# Step 2\n",
    "exhaustive = get_exhaustive_pairs(dataset1, dataset2)\n",
    "print('Number of pairs to perform matching:', len(exhaustive))\n",
    "# Step 3\n",
    "pairwise_sim, run_time_full = score_sim(exhaustive, dataset1, dataset2)\n",
    "print('Matching time (secs):', run_time_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of matches found: 503\n"
     ]
    }
   ],
   "source": [
    "# Step 4\n",
    "matched_results_full = get_match(pairwise_sim, min_score=3)\n",
    "print('Number of matches found:', len(matched_results_full))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_links = load_febrl1(return_links=True)[1]\r\n",
    "# the order within each tuple is not consistent, so we fix this\r\n",
    "links = []\r\n",
    "for idx1, idx2 in loaded_links:\r\n",
    "    links.append((idx2, idx1) if 'dup' in idx1 else (idx1, idx2))\r\n",
    "links = pd.MultiIndex.from_tuples(links)\r\n",
    "#links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_performance(matched_results, true_matches=links, total=250000):\n",
    "    # total is the number of exhaustive pairings (500 * 500 = 250000)\n",
    "    return { \n",
    "            'confusion_matrix': recordlinkage.confusion_matrix(true_matches, matched_results, total),\n",
    "            'recall': recordlinkage.recall(true_matches, matched_results), \n",
    "            'precision': recordlinkage.precision(true_matches, matched_results),\n",
    "            'accuracy':recordlinkage.accuracy(true_matches, matched_results, total)\n",
    "           }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confusion_matrix \n",
      " [[   500      0]\n",
      " [     3 249497]]\n",
      "recall \n",
      " 1.0\n",
      "precision \n",
      " 0.9940357852882704\n",
      "accuracy \n",
      " 0.999988\n"
     ]
    }
   ],
   "source": [
    "for metric, value in check_performance(matched_results_full).items():\n",
    "    print(metric,'\\n', value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reduction ratio:  0.0\n"
     ]
    }
   ],
   "source": [
    "print('Reduction ratio: ', recordlinkage.reduction_ratio(exhaustive, [dataset1, dataset2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Blocking\r\n",
    "\r\n",
    "Change step 2 function to do blocking on a specific column. From the documentation\r\n",
    "\r\n",
    "> The argument `given_name` is the blocking variable. This variable has to be the name of a column in dfA and dfB. It is possible to parse a list of columns names to block on multiple variables. Blocking on multiple variables will reduce the number of record pairs even further.\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_blocking_pairs(blocking_features, dfA, dfB):\n",
    "    \"\"\"\n",
    "    Create an indexing strategy based on a list of `blocking_features`\n",
    "    \"\"\"\n",
    "    indexer = recordlinkage.Index()\n",
    "    for feature in blocking_features:\n",
    "        indexer.block(feature)\n",
    "    return indexer.index(dfA, dfB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of pairs to perform matching: 1181\n",
      "Matching time (secs): 0.06241607666015625\n",
      "Number of matches found (max = 500): 327\n",
      "confusion_matrix \n",
      " [[   326    174]\n",
      " [     1 249499]]\n",
      "recall \n",
      " 0.652\n",
      "precision \n",
      " 0.9969418960244648\n",
      "accuracy \n",
      " 0.9993\n"
     ]
    }
   ],
   "source": [
    "given_name_blocking = get_blocking_pairs(['given_name'], dataset1, dataset2)\n",
    "print('Number of pairs to perform matching:', len(given_name_blocking))\n",
    "\n",
    "pairwise_sim, given_name_run_time = score_sim(given_name_blocking, dataset1, dataset2)\n",
    "print('Matching time (secs):', given_name_run_time)\n",
    "\n",
    "matched_results = get_match(pairwise_sim, min_score=3)\n",
    "print('Number of matches found (max = 500):', len(matched_results))\n",
    "\n",
    "for metric, value in check_performance(matched_results).items():\n",
    "    print(metric,'\\n', value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reduction ratio:  0.995276\n"
     ]
    }
   ],
   "source": [
    "print('Reduction ratio: ', recordlinkage.reduction_ratio(given_name_blocking, [dataset1, dataset2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, it is worse, because even though accuracy is the same, the number of matches is reduced, and there are fewer true-positives. It is much faster but performance is also worse. Accuracy is not very useful because the number of false negative is huge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improving the Blocking Strategy\n",
    "\n",
    "Another technique we can try to improve the naive blocking strategy, beside manipulating the `min_score`, `threshold` and `blocking_features` parameters, is to create some new blocking features to improve the chance of potential matches being compared. This is known as feature engineering.\n",
    "\n",
    "In the following section, we will create 2 new blocking features: the prefixes of surname and given name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset1['surname_prefix'] = dataset1['surname'].apply(lambda x: re.findall(r'^.{1,4}', str(x))[0])\r\n",
    "dataset2['surname_prefix'] = dataset2['surname'].apply(lambda x: re.findall(r'^.{1,4}', str(x))[0])\r\n",
    "dataset1['given_name_prefix'] = dataset1['given_name'].apply(lambda x: re.findall(r'^.{1,4}', str(x))[0])\r\n",
    "dataset2['given_name_prefix'] = dataset2['given_name'].apply(lambda x: re.findall(r'^.{1,4}', str(x))[0])\r\n",
    "\r\n",
    "# Revision question: What is the regex pattern finding? Print the dataset1 and dataset2 to confirm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of pairs to perform matching: 2898\n",
      "Matching time (secs): 0.12570619583129883\n",
      "Number of matches found: 445\n",
      "confusion_matrix \n",
      " [[   442     58]\n",
      " [     3 249497]]\n",
      "recall \n",
      " 0.884\n",
      "precision \n",
      " 0.9932584269662922\n",
      "accuracy \n",
      " 0.999756\n"
     ]
    }
   ],
   "source": [
    "## blocking on prefixes of surname and given_name\n",
    "prefix_blocking = get_blocking_pairs(['surname_prefix', 'given_name_prefix'], dataset1, dataset2)\n",
    "\n",
    "print('Number of pairs to perform matching:', len(prefix_blocking))\n",
    "\n",
    "pairwise_sim, run_time_with_block = score_sim(prefix_blocking, dataset1, dataset2)\n",
    "print('Matching time (secs):', run_time_with_block)\n",
    "\n",
    "matched_results = get_match(pairwise_sim, min_score=3)\n",
    "print('Number of matches found:', len(matched_results))\n",
    "\n",
    "for metric, value in check_performance(matched_results).items():\n",
    "    print(metric,'\\n', value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reduction ratio:  0.988408\n"
     ]
    }
   ],
   "source": [
    "print('Reduction ratio: ', recordlinkage.reduction_ratio(prefix_blocking, [dataset1, dataset2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing no-blocking and this improved blocking\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "performance_full = check_performance(matched_results_full)\n",
    "performance_with_block = check_performance(matched_results)\n",
    "performance_full['run-time'] = run_time_full\n",
    "performance_full['pair-count'] = len(exhaustive)\n",
    "performance_with_block['run-time'] = run_time_with_block\n",
    "performance_with_block['pair-count'] = len(prefix_blocking)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exhaustive comparison:\n",
      "[[   500      0]\n",
      " [     3 249497]]\n",
      "\tprecision: 0.9940357852882704\n",
      "\trecall: 1.0\n",
      "\trun-time: 9.239\n",
      "\tpair-count: 250000\n",
      "\n",
      "With improved blocking:\n",
      "[[   442     58]\n",
      " [     3 249497]]\n",
      "\tprecision: 0.9932584269662922\n",
      "\trecall: 0.884\n",
      "\trun-time: 0.126, 1.36% of exhaustive comparison\n",
      "\tpair-count: 2898, 1.16% of exhaustive comparison\n"
     ]
    }
   ],
   "source": [
    "print('Exhaustive comparison:')\n",
    "print(f\"{performance_full['confusion_matrix']}\")\n",
    "print(f\"\\tprecision: {performance_full['precision']}\")\n",
    "print(f\"\\trecall: {performance_full['recall']}\")\n",
    "print(f\"\\trun-time: {round(performance_full['run-time'], 3)}\")\n",
    "print(f\"\\tpair-count: {performance_full['pair-count']}\")\n",
    "\n",
    "print('\\nWith improved blocking:')\n",
    "print(f\"{performance_with_block['confusion_matrix']}\")\n",
    "print(f\"\\tprecision: {performance_with_block['precision']}\")\n",
    "print(f\"\\trecall: {performance_with_block['recall']}\")\n",
    "print(f\"\\trun-time: {round(performance_with_block['run-time'], 3)}, {round(100*performance_with_block['run-time']/performance_full['run-time'], 2)}% of exhaustive comparison\")\n",
    "print(f\"\\tpair-count: {performance_with_block['pair-count']}, {round(100*performance_with_block['pair-count']/performance_full['pair-count'], 2)}% of exhaustive comparison\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bloom Filters\r\n",
    "\r\n",
    "The following section will introduce you to a Python implementation of bloom filters, using the `mmh3` library to implement the MurmurHash (MurmurHash3) hashing function. Note that you can use other hashing functions for bloom filters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bloom_filter(string1, string2, I, k):\r\n",
    "    \r\n",
    "    \"\"\"\r\n",
    "    Return two I-length bloom filters for string1 and string2 using k MMH3 hash functions\r\n",
    "    \"\"\"\r\n",
    "    \r\n",
    "    # Create a string into a set of smaller strings (bigrams)\r\n",
    "    bigrams1 = [''.join(e) for e in bigrams(string1)]\r\n",
    "    bigrams2 = [''.join(e) for e in bigrams(string2)]\r\n",
    "    \r\n",
    "    # Initialize the bloom filters\r\n",
    "    bloomfilter1 = [0]*I\r\n",
    "    bloomfilter2 = [0]*I\r\n",
    "\r\n",
    "    for i in range(k):\r\n",
    "    # For each hashing function, apply it to each element of the bigram lists, and update the according index to 1\r\n",
    "        for w1 in bigrams1:\r\n",
    "            idx1 = mmh3.hash(w1, seed=i) % I # Question: Why use the modulo function here?\r\n",
    "            bloomfilter1[idx1] = 1\r\n",
    "        for w2 in bigrams2:\r\n",
    "            idx2 = mmh3.hash(w2, seed=i) % I\r\n",
    "            bloomfilter2[idx2] = 1\r\n",
    "\r\n",
    "    return bloomfilter1, bloomfilter2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bloom filter for _SMITH_: [0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1]\n",
      "Bloom filter for _SMYTH_: [0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "bloomfilter1, bloomfilter2 = bloom_filter('_SMITH_', '_SMYTH_', I = 30, k = 2)\n",
    "\n",
    "print('Bloom filter for _SMITH_:', bloomfilter1)\n",
    "print('Bloom filter for _SMYTH_:', bloomfilter2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use bloom filters to calculate the Dice similarity scores\r\n",
    "def dice_sim_bloomfilter(string1, string2, I, k):\r\n",
    "    ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\r\n",
    "\r\n",
    "    print(dice_sim_bloomfilter('_SMITH_', '_SMYTH_', I = 30, k = 2))\r\n",
    "    0.6666666666666666\r\n",
    "    print(dice_sim_bloomfilter('_SMITH_', '_SMITH_', I = 30, k = 2))\r\n",
    "    1.0\r\n",
    "    print(dice_sim_bloomfilter('_SMITH_', '_SMOTH_', I = 30, k = 2))\r\n",
    "    0.7619047619047619\r\n",
    "    print(dice_sim_bloomfilter('_SMOTH_', '_SMYTH_', I = 30, k = 2))\r\n",
    "    0.6363636363636364"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.0 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "2d04f120ef9720f2488447f7ea0097f595927923387bca235b2adec1aebe5795"
    }
   },
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}