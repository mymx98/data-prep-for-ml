{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case Study Background\n",
    "The marketing team is having trouble joining 2 customer datasets from 2 different marketing campaigns as some user inputs were erroneous. They have identified that each customer from dataset 1 has exactly 1 match in dataset 2. You're tasked with performing data linkage to identify these matches.\n",
    "\n",
    "### About the dataset\n",
    "\n",
    "The datasets used in this worksheet is obtained from the `recordlinkage` library. It contains 2 dataframes of size 500, consisting of 8 personal detail fields. The record with index `rec-0-org` from dataset 1 has a match with the record with index `rec-0-dup-0` from dataset 2; the record with index `rec-1-org` from dataset 1 has a match with the record with index `rec-1-dup-0` from dataset 2; and so on...\n",
    "\n",
    "For the sake of learning, we will pretend that we don't know that these are matches, and will see how many of these matches are returned by the linkage algorithms.\n",
    "\n",
    "## Learning objectives\n",
    "- Understand the concept of data linkage, blocking\n",
    "- Know how to evaluate the performance of a linkage algorithm\n",
    "- Know how to improve the performance of a linkage algorithm via blocking\n",
    "- Know how to perform basic blocking using the `recordlinkage` library\n",
    "\n",
    "## Workshop Overview\n",
    "- Investigate the functions to generate the index pairs (exhaustive matching and blocking)\n",
    "- Investigate the function to compare the records\n",
    "- Calculate the performance metrics of a linkage algorithm\n",
    "- Compare the exhaustive matching vs blocked matching\n",
    "- Create new blocking features with feature engineering\n",
    "- Answer theoretical questions related to the concepts practiced in the case study\n",
    "\n",
    "## Acknowledgement / References\n",
    "https://recordlinkage.readthedocs.io/en/latest/notebooks/link_two_dataframes.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import recordlinkage\n",
    "import time\n",
    "# pip install recordlinkage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>given_name</th>\n",
       "      <th>surname</th>\n",
       "      <th>street_number</th>\n",
       "      <th>address_1</th>\n",
       "      <th>suburb</th>\n",
       "      <th>postcode</th>\n",
       "      <th>state</th>\n",
       "      <th>date_of_birth</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rec_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>rec-0-org</th>\n",
       "      <td>flynn</td>\n",
       "      <td>rokobaro</td>\n",
       "      <td>12</td>\n",
       "      <td>herschell circuit</td>\n",
       "      <td>lawrence</td>\n",
       "      <td>2227</td>\n",
       "      <td>nsw</td>\n",
       "      <td>19720812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rec-1-org</th>\n",
       "      <td>karli</td>\n",
       "      <td>alderson</td>\n",
       "      <td>144</td>\n",
       "      <td>nulsen circuit</td>\n",
       "      <td>tingalpa</td>\n",
       "      <td>3139</td>\n",
       "      <td>nsw</td>\n",
       "      <td>19510826</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          given_name   surname  street_number          address_1    suburb  \\\n",
       "rec_id                                                                       \n",
       "rec-0-org      flynn  rokobaro             12  herschell circuit  lawrence   \n",
       "rec-1-org      karli  alderson            144     nulsen circuit  tingalpa   \n",
       "\n",
       "          postcode state date_of_birth  \n",
       "rec_id                                  \n",
       "rec-0-org     2227   nsw      19720812  \n",
       "rec-1-org     3139   nsw      19510826  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>given_name</th>\n",
       "      <th>surname</th>\n",
       "      <th>street_number</th>\n",
       "      <th>address_1</th>\n",
       "      <th>suburb</th>\n",
       "      <th>postcode</th>\n",
       "      <th>state</th>\n",
       "      <th>date_of_birth</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rec_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>rec-0-dup-0</th>\n",
       "      <td>thomas</td>\n",
       "      <td>rokobaro</td>\n",
       "      <td>12</td>\n",
       "      <td>herschell circuit</td>\n",
       "      <td>lawrence</td>\n",
       "      <td>2272</td>\n",
       "      <td>nsw</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rec-1-dup-0</th>\n",
       "      <td>karli</td>\n",
       "      <td>alderson</td>\n",
       "      <td>144</td>\n",
       "      <td>nulsen circuit</td>\n",
       "      <td>tings lpa</td>\n",
       "      <td>3139</td>\n",
       "      <td>nsw</td>\n",
       "      <td>19510826.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            given_name   surname  street_number          address_1     suburb  \\\n",
       "rec_id                                                                          \n",
       "rec-0-dup-0     thomas  rokobaro             12  herschell circuit   lawrence   \n",
       "rec-1-dup-0      karli  alderson            144     nulsen circuit  tings lpa   \n",
       "\n",
       "            postcode state date_of_birth  \n",
       "rec_id                                    \n",
       "rec-0-dup-0     2272   nsw           nan  \n",
       "rec-1-dup-0     3139   nsw    19510826.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load toy datasets\n",
    "sample1 = pd.read_csv('datalinkage_sample1.csv').set_index('rec_id')\n",
    "sample2 = pd.read_csv('datalinkage_sample2.csv').set_index('rec_id')\n",
    "sample1['date_of_birth'] = sample1['date_of_birth'].astype(str)\n",
    "sample2['date_of_birth'] = sample2['date_of_birth'].astype(str)\n",
    "sample1['postcode'] = sample1['postcode'].astype(str)\n",
    "sample2['postcode'] = sample2['postcode'].astype(str)\n",
    "display(sample1)\n",
    "display(sample2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <u> Concept: Data Linkage </u>\n",
    "\n",
    "A data linkage / record matching pipeline between dataset A and B usually follows 4 steps:\n",
    "\n",
    "1. Pre-process A and B & determine the features/attributes to calculate the similarity score\n",
    "2. Generate a list of (indexA, indexB) to compare against one another. The strategy to generate the list can either be exhaustive (every record in A against every record in B); random; or via blocking.\n",
    "3. For each (indexA, indexB), calculate the similarity score\n",
    "4. Choose a threshold. If the score is >= this threshold, then indexA and indexB is a match.\n",
    "\n",
    "For this tutorial, both datasets have been pre-processed, so we can skip step 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Generate the index pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:recordlinkage:indexing - performance warning - A full index can result in large number of record pairs.\n",
      "MultiIndex([('rec-0-org', 'rec-0-dup-0'),\n",
      "            ('rec-0-org', 'rec-1-dup-0'),\n",
      "            ('rec-1-org', 'rec-0-dup-0'),\n",
      "            ('rec-1-org', 'rec-1-dup-0')],\n",
      "           names=['rec_id_1', 'rec_id_2'])\n",
      "Number of pairs to perform matching: 4\n"
     ]
    }
   ],
   "source": [
    "def get_exhaustive_pairs(dfA, dfB):\n",
    "    \n",
    "    indexer = recordlinkage.Index()\n",
    "    indexer.full()\n",
    "    return indexer.index(dfA, dfB)\n",
    "\n",
    "exhaustive_sample = get_exhaustive_pairs(sample1, sample2)\n",
    "print(exhaustive_sample)\n",
    "print('Number of pairs to perform matching:', len(exhaustive_sample))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Calculate the similarity score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_sim(index_pairs, dfA, dfB):\n",
    "    \n",
    "    \"\"\"\n",
    "    Perform linkage based on the index_pairs generated in step 2.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Instantiate the scorer\n",
    "    scorer = recordlinkage.Compare()\n",
    "    \n",
    "    # Define the scoring function for each feature. See https://recordlinkage.readthedocs.io/en/latest/ref-compare.html#module-recordlinkage.compare\n",
    "    scorer.string('given_name', 'given_name', method='levenshtein', threshold=0.85, label='given_name')\n",
    "    scorer.string('surname', 'surname', method='levenshtein', threshold=0.85, label='surname')\n",
    "    scorer.string('date_of_birth', 'date_of_birth',  method='levenshtein', threshold=0.75, label='date_of_birth')\n",
    "    scorer.exact('suburb', 'suburb', label='suburb')\n",
    "    scorer.exact('state', 'state', label='state')\n",
    "    scorer.string('postcode', 'postcode', label='postcode')\n",
    "    scorer.string('address_1', 'address_1', method='levenshtein', threshold=0.85, label='address_1')\n",
    "    \n",
    "    # Perform the scoring and get the runtime for performance evaluation\n",
    "    start = time.time()\n",
    "    pairwise_sim = scorer.compute(index_pairs, dfA, dfB)\n",
    "    end = time.time()\n",
    "    runtime = end- start\n",
    "    \n",
    "    return pairwise_sim, runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matching time (secs): 0.013808012008666992\n"
     ]
    }
   ],
   "source": [
    "pairwise_sim, run_time = score_sim(exhaustive_sample, sample1, sample2)\n",
    "print('Matching time (secs):', run_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Compare against threshold and determining the matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>given_name</th>\n",
       "      <th>surname</th>\n",
       "      <th>date_of_birth</th>\n",
       "      <th>suburb</th>\n",
       "      <th>state</th>\n",
       "      <th>postcode</th>\n",
       "      <th>address_1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rec_id_1</th>\n",
       "      <th>rec_id_2</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">rec-0-org</th>\n",
       "      <th>rec-0-dup-0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rec-1-dup-0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">rec-1-org</th>\n",
       "      <th>rec-0-dup-0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rec-1-dup-0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       given_name  surname  date_of_birth  suburb  state  \\\n",
       "rec_id_1  rec_id_2                                                         \n",
       "rec-0-org rec-0-dup-0         0.0      1.0            0.0       1      1   \n",
       "          rec-1-dup-0         0.0      0.0            0.0       0      1   \n",
       "rec-1-org rec-0-dup-0         0.0      0.0            0.0       0      1   \n",
       "          rec-1-dup-0         1.0      1.0            1.0       0      1   \n",
       "\n",
       "                       postcode  address_1  \n",
       "rec_id_1  rec_id_2                          \n",
       "rec-0-org rec-0-dup-0       0.5        1.0  \n",
       "          rec-1-dup-0       0.0        0.0  \n",
       "rec-1-org rec-0-dup-0       0.0        0.0  \n",
       "          rec-1-dup-0       1.0        1.0  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Investigate the output\n",
    "pairwise_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rec_id_1   rec_id_2   \n",
       "rec-1-org  rec-1-dup-0    6.0\n",
       "rec-0-org  rec-0-dup-0    4.5\n",
       "           rec-1-dup-0    1.0\n",
       "rec-1-org  rec-0-dup-0    1.0\n",
       "dtype: float64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Row sum and sort\n",
    "pairwise_sim.sum(axis=1).sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultiIndex([('rec-0-org', 'rec-0-dup-0'),\n",
       "            ('rec-1-org', 'rec-1-dup-0')],\n",
       "           names=['rec_id_1', 'rec_id_2'])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_match(pairwise_sim, min_score=4):\n",
    "    scores = pairwise_sim.sum(axis=1)\n",
    "    return scores[scores>=min_score].index\n",
    "\n",
    "matched_results = get_match(pairwise_sim)\n",
    "matched_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try it on the full dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset1 = pd.read_csv('datalinkage_linkage1.csv').set_index('rec_id') # 500 rows\n",
    "dataset2 = pd.read_csv('datalinkage_linkage2.csv').set_index('rec_id') # 500 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset1['date_of_birth'] = dataset1['date_of_birth'].astype(str)\n",
    "dataset2['date_of_birth'] = dataset2['date_of_birth'].astype(str)\n",
    "dataset1['postcode'] = dataset1['postcode'].astype(str)\n",
    "dataset2['postcode'] = dataset2['postcode'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:recordlinkage:indexing - performance warning - A full index can result in large number of record pairs.\n",
      "Number of pairs to perform matching: 250000\n",
      "Matching time (secs): 8.873044967651367\n"
     ]
    }
   ],
   "source": [
    "# Step 2\n",
    "exhaustive = get_exhaustive_pairs(dataset1, dataset2)\n",
    "print('Number of pairs to perform matching:', len(exhaustive))\n",
    "# Step 3\n",
    "pairwise_sim, run_time_full = score_sim(exhaustive, dataset1, dataset2)\n",
    "print('Matching time (secs):', run_time_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of matches found: 503\n"
     ]
    }
   ],
   "source": [
    "# Step 4\n",
    "matched_results_full = get_match(pairwise_sim, min_score=3)\n",
    "print('Number of matches found:', len(matched_results_full))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the true matches for this linkage dataset (courtesy of https://recordlinkage.readthedocs.io/en/latest/notebooks/link_two_dataframes.html)\n",
    "from recordlinkage.datasets import load_febrl1\n",
    "loaded_links = load_febrl1(return_links=True)[1]\n",
    "# the order within each tuple is not consistent, so we fix this\n",
    "links = []\n",
    "for idx1, idx2 in loaded_links:\n",
    "    links.append((idx2, idx1) if 'dup' in idx1 else (idx1, idx2))\n",
    "links = pd.MultiIndex.from_tuples(links)\n",
    "#links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_performance(matched_results, true_matches=links, total=250000):\n",
    "    # total is the number of exhaustive pairings (500 * 500 = 250000)\n",
    "    return { \n",
    "            'confusion_matrix': recordlinkage.confusion_matrix(true_matches, matched_results, total),\n",
    "            'recall': recordlinkage.recall(true_matches, matched_results), \n",
    "            'precision': recordlinkage.precision(true_matches, matched_results),\n",
    "            'accuracy':recordlinkage.accuracy(true_matches, matched_results, total)\n",
    "           }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confusion_matrix \n",
      " [[   500      0]\n",
      " [     3 249497]]\n",
      "recall \n",
      " 1.0\n",
      "precision \n",
      " 0.9940357852882704\n",
      "accuracy \n",
      " 0.999988\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.10/site-packages/recordlinkage/measures.py:147: FutureWarning: Index.__and__ operating as a set operation is deprecated, in the future this will be a logical operation matching Series.__and__.  Use index.intersection(other) instead.\n",
      "  return len(links_true & links_pred)\n",
      "/usr/lib/python3.10/site-packages/recordlinkage/measures.py:179: FutureWarning: Index.__or__ operating as a set operation is deprecated, in the future this will be a logical operation matching Series.__or__.  Use index.union(other) instead.\n",
      "  return int(total) - len(links_true | links_pred)\n",
      "/usr/lib/python3.10/site-packages/recordlinkage/measures.py:147: FutureWarning: Index.__and__ operating as a set operation is deprecated, in the future this will be a logical operation matching Series.__and__.  Use index.intersection(other) instead.\n",
      "  return len(links_true & links_pred)\n",
      "/usr/lib/python3.10/site-packages/recordlinkage/measures.py:147: FutureWarning: Index.__and__ operating as a set operation is deprecated, in the future this will be a logical operation matching Series.__and__.  Use index.intersection(other) instead.\n",
      "  return len(links_true & links_pred)\n",
      "/usr/lib/python3.10/site-packages/recordlinkage/measures.py:147: FutureWarning: Index.__and__ operating as a set operation is deprecated, in the future this will be a logical operation matching Series.__and__.  Use index.intersection(other) instead.\n",
      "  return len(links_true & links_pred)\n",
      "/usr/lib/python3.10/site-packages/recordlinkage/measures.py:179: FutureWarning: Index.__or__ operating as a set operation is deprecated, in the future this will be a logical operation matching Series.__or__.  Use index.union(other) instead.\n",
      "  return int(total) - len(links_true | links_pred)\n"
     ]
    }
   ],
   "source": [
    "for metric, value in check_performance(matched_results_full).items():\n",
    "    print(metric,'\\n', value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reduction ratio:  0.0\n"
     ]
    }
   ],
   "source": [
    "print('Reduction ratio: ', recordlinkage.reduction_ratio(exhaustive, [dataset1, dataset2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<blockquote style=\"padding: 10px; background-color: #ebf5fb;\">\n",
    "\n",
    "    \n",
    "## Discussion questions\n",
    "1. How do we interpret the confusion matrix?\n",
    "2. What is the reduction ratio? What does it tell us about the current indexing strategy?\n",
    "3. How did our matching criteria perform?\n",
    "4. How can we reduce the run time without negatively impacting the current performance?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <u> Concept: Blocking </u>\n",
    "\n",
    "\n",
    "Change step 2 function to do blocking on a specific column. From the documentation\n",
    "\n",
    "> The argument `given_name` is the blocking variable. This variable has to be the name of a column in dfA and dfB. It is possible to parse a list of columns names to block on multiple variables. Blocking on multiple variables will reduce the number of record pairs even further.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_blocking_pairs(blocking_features, dfA, dfB):\n",
    "    \"\"\"\n",
    "    Create an indexing strategy based on a list of `blocking_features`\n",
    "    \"\"\"\n",
    "    indexer = recordlinkage.Index()\n",
    "    for feature in blocking_features:\n",
    "        indexer.block(feature)\n",
    "    return indexer.index(dfA, dfB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.10/site-packages/recordlinkage/measures.py:147: FutureWarning: Index.__and__ operating as a set operation is deprecated, in the future this will be a logical operation matching Series.__and__.  Use index.intersection(other) instead.\n",
      "  return len(links_true & links_pred)\n",
      "/usr/lib/python3.10/site-packages/recordlinkage/measures.py:179: FutureWarning: Index.__or__ operating as a set operation is deprecated, in the future this will be a logical operation matching Series.__or__.  Use index.union(other) instead.\n",
      "  return int(total) - len(links_true | links_pred)\n",
      "/usr/lib/python3.10/site-packages/recordlinkage/measures.py:147: FutureWarning: Index.__and__ operating as a set operation is deprecated, in the future this will be a logical operation matching Series.__and__.  Use index.intersection(other) instead.\n",
      "  return len(links_true & links_pred)\n",
      "/usr/lib/python3.10/site-packages/recordlinkage/measures.py:147: FutureWarning: Index.__and__ operating as a set operation is deprecated, in the future this will be a logical operation matching Series.__and__.  Use index.intersection(other) instead.\n",
      "  return len(links_true & links_pred)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of pairs to perform matching: 1181\n",
      "Matching time (secs): 0.05480313301086426\n",
      "Number of matches found (max = 500): 327\n",
      "confusion_matrix \n",
      " [[   326    174]\n",
      " [     1 249499]]\n",
      "recall \n",
      " 0.652\n",
      "precision \n",
      " 0.9969418960244648\n",
      "accuracy \n",
      " 0.9993\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.10/site-packages/recordlinkage/measures.py:147: FutureWarning: Index.__and__ operating as a set operation is deprecated, in the future this will be a logical operation matching Series.__and__.  Use index.intersection(other) instead.\n",
      "  return len(links_true & links_pred)\n",
      "/usr/lib/python3.10/site-packages/recordlinkage/measures.py:179: FutureWarning: Index.__or__ operating as a set operation is deprecated, in the future this will be a logical operation matching Series.__or__.  Use index.union(other) instead.\n",
      "  return int(total) - len(links_true | links_pred)\n"
     ]
    }
   ],
   "source": [
    "given_name_blocking = get_blocking_pairs(['given_name'], dataset1, dataset2)\n",
    "print('Number of pairs to perform matching:', len(given_name_blocking))\n",
    "\n",
    "pairwise_sim, given_name_run_time = score_sim(given_name_blocking, dataset1, dataset2)\n",
    "print('Matching time (secs):', given_name_run_time)\n",
    "\n",
    "matched_results = get_match(pairwise_sim, min_score=3)\n",
    "print('Number of matches found (max = 500):', len(matched_results))\n",
    "\n",
    "for metric, value in check_performance(matched_results).items():\n",
    "    print(metric,'\\n', value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reduction ratio:  0.995276\n"
     ]
    }
   ],
   "source": [
    "print('Reduction ratio: ', recordlinkage.reduction_ratio(given_name_blocking, [dataset1, dataset2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<blockquote style=\"padding: 10px; background-color: #ebf5fb;\">\n",
    "\n",
    "    \n",
    "## Discussion question\n",
    "Compared to the exhaustive matching strategy, accuracy of the blocking by given_name is similar, but it's much faster. Does this mean the performance of blocking is superior to the exhaustive matching?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer: In this case, it is worse, because even though accuracy is the same, the number of matches is reduced, and there are fewer true-positives.\n",
    "    It is much faster but performance is also worse.\n",
    "    Accuracy is not very useful because the number of false negative is huge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<blockquote style=\"padding: 10px; background-color: #FFD392;\">\n",
    "\n",
    "\n",
    "## Exercise\n",
    "Change some parameters, like `min_score`, `threshold` and `blocking_features` to see how it affects the performance of the matching algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improving the Blocking Strategy\n",
    "\n",
    "Another technique we can try to improve the naive blocking strategy, beside manipulating the `min_score`, `threshold` and `blocking_features` parameters, is to create some new blocking features to improve the chance of potential matches being compared. This is known as feature engineering.\n",
    "\n",
    "In the following section, we will create 2 new blocking features: the prefixes of surname and given name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "dataset1['surname_prefix'] = dataset1['surname'].apply(lambda x: re.findall(r'^.{1,4}', str(x))[0])\n",
    "dataset2['surname_prefix'] = dataset2['surname'].apply(lambda x: re.findall(r'^.{1,4}', str(x))[0])\n",
    "dataset1['given_name_prefix'] = dataset1['given_name'].apply(lambda x: re.findall(r'^.{1,4}', str(x))[0])\n",
    "dataset2['given_name_prefix'] = dataset2['given_name'].apply(lambda x: re.findall(r'^.{1,4}', str(x))[0])\n",
    "\n",
    "# Revision question: What is the regex pattern finding? Print the dataset1 and dataset2 to confirm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of pairs to perform matching: 2898\n",
      "Matching time (secs): 0.12687373161315918\n",
      "Number of matches found: 445\n",
      "confusion_matrix \n",
      " [[   442     58]\n",
      " [     3 249497]]\n",
      "recall \n",
      " 0.884\n",
      "precision \n",
      " 0.9932584269662922\n",
      "accuracy \n",
      " 0.999756\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.10/site-packages/recordlinkage/measures.py:147: FutureWarning: Index.__and__ operating as a set operation is deprecated, in the future this will be a logical operation matching Series.__and__.  Use index.intersection(other) instead.\n",
      "  return len(links_true & links_pred)\n",
      "/usr/lib/python3.10/site-packages/recordlinkage/measures.py:179: FutureWarning: Index.__or__ operating as a set operation is deprecated, in the future this will be a logical operation matching Series.__or__.  Use index.union(other) instead.\n",
      "  return int(total) - len(links_true | links_pred)\n",
      "/usr/lib/python3.10/site-packages/recordlinkage/measures.py:147: FutureWarning: Index.__and__ operating as a set operation is deprecated, in the future this will be a logical operation matching Series.__and__.  Use index.intersection(other) instead.\n",
      "  return len(links_true & links_pred)\n",
      "/usr/lib/python3.10/site-packages/recordlinkage/measures.py:147: FutureWarning: Index.__and__ operating as a set operation is deprecated, in the future this will be a logical operation matching Series.__and__.  Use index.intersection(other) instead.\n",
      "  return len(links_true & links_pred)\n",
      "/usr/lib/python3.10/site-packages/recordlinkage/measures.py:147: FutureWarning: Index.__and__ operating as a set operation is deprecated, in the future this will be a logical operation matching Series.__and__.  Use index.intersection(other) instead.\n",
      "  return len(links_true & links_pred)\n",
      "/usr/lib/python3.10/site-packages/recordlinkage/measures.py:179: FutureWarning: Index.__or__ operating as a set operation is deprecated, in the future this will be a logical operation matching Series.__or__.  Use index.union(other) instead.\n",
      "  return int(total) - len(links_true | links_pred)\n"
     ]
    }
   ],
   "source": [
    "## blocking on prefixes of surname and given_name\n",
    "prefix_blocking = get_blocking_pairs(['surname_prefix', 'given_name_prefix'], dataset1, dataset2)\n",
    "\n",
    "print('Number of pairs to perform matching:', len(prefix_blocking))\n",
    "\n",
    "pairwise_sim, run_time_with_block = score_sim(prefix_blocking, dataset1, dataset2)\n",
    "print('Matching time (secs):', run_time_with_block)\n",
    "\n",
    "matched_results = get_match(pairwise_sim, min_score=3)\n",
    "print('Number of matches found:', len(matched_results))\n",
    "\n",
    "for metric, value in check_performance(matched_results).items():\n",
    "    print(metric,'\\n', value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reduction ratio:  0.988408\n"
     ]
    }
   ],
   "source": [
    "print('Reduction ratio: ', recordlinkage.reduction_ratio(prefix_blocking, [dataset1, dataset2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing no-blocking and this improved blocking\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.10/site-packages/recordlinkage/measures.py:147: FutureWarning: Index.__and__ operating as a set operation is deprecated, in the future this will be a logical operation matching Series.__and__.  Use index.intersection(other) instead.\n",
      "  return len(links_true & links_pred)\n",
      "/usr/lib/python3.10/site-packages/recordlinkage/measures.py:179: FutureWarning: Index.__or__ operating as a set operation is deprecated, in the future this will be a logical operation matching Series.__or__.  Use index.union(other) instead.\n",
      "  return int(total) - len(links_true | links_pred)\n",
      "/usr/lib/python3.10/site-packages/recordlinkage/measures.py:147: FutureWarning: Index.__and__ operating as a set operation is deprecated, in the future this will be a logical operation matching Series.__and__.  Use index.intersection(other) instead.\n",
      "  return len(links_true & links_pred)\n",
      "/usr/lib/python3.10/site-packages/recordlinkage/measures.py:147: FutureWarning: Index.__and__ operating as a set operation is deprecated, in the future this will be a logical operation matching Series.__and__.  Use index.intersection(other) instead.\n",
      "  return len(links_true & links_pred)\n",
      "/usr/lib/python3.10/site-packages/recordlinkage/measures.py:147: FutureWarning: Index.__and__ operating as a set operation is deprecated, in the future this will be a logical operation matching Series.__and__.  Use index.intersection(other) instead.\n",
      "  return len(links_true & links_pred)\n",
      "/usr/lib/python3.10/site-packages/recordlinkage/measures.py:179: FutureWarning: Index.__or__ operating as a set operation is deprecated, in the future this will be a logical operation matching Series.__or__.  Use index.union(other) instead.\n",
      "  return int(total) - len(links_true | links_pred)\n",
      "/usr/lib/python3.10/site-packages/recordlinkage/measures.py:147: FutureWarning: Index.__and__ operating as a set operation is deprecated, in the future this will be a logical operation matching Series.__and__.  Use index.intersection(other) instead.\n",
      "  return len(links_true & links_pred)\n",
      "/usr/lib/python3.10/site-packages/recordlinkage/measures.py:179: FutureWarning: Index.__or__ operating as a set operation is deprecated, in the future this will be a logical operation matching Series.__or__.  Use index.union(other) instead.\n",
      "  return int(total) - len(links_true | links_pred)\n",
      "/usr/lib/python3.10/site-packages/recordlinkage/measures.py:147: FutureWarning: Index.__and__ operating as a set operation is deprecated, in the future this will be a logical operation matching Series.__and__.  Use index.intersection(other) instead.\n",
      "  return len(links_true & links_pred)\n",
      "/usr/lib/python3.10/site-packages/recordlinkage/measures.py:147: FutureWarning: Index.__and__ operating as a set operation is deprecated, in the future this will be a logical operation matching Series.__and__.  Use index.intersection(other) instead.\n",
      "  return len(links_true & links_pred)\n",
      "/usr/lib/python3.10/site-packages/recordlinkage/measures.py:147: FutureWarning: Index.__and__ operating as a set operation is deprecated, in the future this will be a logical operation matching Series.__and__.  Use index.intersection(other) instead.\n",
      "  return len(links_true & links_pred)\n",
      "/usr/lib/python3.10/site-packages/recordlinkage/measures.py:179: FutureWarning: Index.__or__ operating as a set operation is deprecated, in the future this will be a logical operation matching Series.__or__.  Use index.union(other) instead.\n",
      "  return int(total) - len(links_true | links_pred)\n"
     ]
    }
   ],
   "source": [
    "performance_full = check_performance(matched_results_full)\n",
    "performance_with_block = check_performance(matched_results)\n",
    "performance_full['run-time'] = run_time_full\n",
    "performance_full['pair-count'] = len(exhaustive)\n",
    "performance_with_block['run-time'] = run_time_with_block\n",
    "performance_with_block['pair-count'] = len(prefix_blocking)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exhaustive comparison:\n",
      "[[   500      0]\n",
      " [     3 249497]]\n",
      "\tprecision: 0.9940357852882704\n",
      "\trecall: 1.0\n",
      "\trun-time: 8.873\n",
      "\tpair-count: 250000\n",
      "\n",
      "With improved blocking:\n",
      "[[   442     58]\n",
      " [     3 249497]]\n",
      "\tprecision: 0.9932584269662922\n",
      "\trecall: 0.884\n",
      "\trun-time: 0.127, 1.43% of exhaustive comparison\n",
      "\tpair-count: 2898, 1.16% of exhaustive comparison\n"
     ]
    }
   ],
   "source": [
    "print('Exhaustive comparison:')\n",
    "print(f\"{performance_full['confusion_matrix']}\")\n",
    "print(f\"\\tprecision: {performance_full['precision']}\")\n",
    "print(f\"\\trecall: {performance_full['recall']}\")\n",
    "print(f\"\\trun-time: {round(performance_full['run-time'], 3)}\")\n",
    "print(f\"\\tpair-count: {performance_full['pair-count']}\")\n",
    "\n",
    "print('\\nWith improved blocking:')\n",
    "print(f\"{performance_with_block['confusion_matrix']}\")\n",
    "print(f\"\\tprecision: {performance_with_block['precision']}\")\n",
    "print(f\"\\trecall: {performance_with_block['recall']}\")\n",
    "print(f\"\\trun-time: {round(performance_with_block['run-time'], 3)}, {round(100*performance_with_block['run-time']/performance_full['run-time'], 2)}% of exhaustive comparison\")\n",
    "print(f\"\\tpair-count: {performance_with_block['pair-count']}, {round(100*performance_with_block['pair-count']/performance_full['pair-count'], 2)}% of exhaustive comparison\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following plots show how increasing the size of the datasets affect the run time performance of each strategy (complexity analysis)\n",
    "![](./efficiency_noblocking.png)\n",
    "![](./efficiency_blocking.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<blockquote style=\"padding: 10px; background-color: #ebf5fb;\">\n",
    "\n",
    "    \n",
    "## Theoretical questions\n",
    "Suppose you are conducting data linkage between two databases, one with $m$ records and the other with $n$ records (assume $m \\le n$). Under a basic (exhaustive) approach, $m \\times n$ record comparisons will be needed. Assuming that there are no duplicates:\n",
    "\n",
    "1. Assume there are no duplicates. What is the maximum number of record matches? What is the corresponding number of non-matching comparisons required in this circumstance?\n",
    "\n",
    "Now suppose a blocking method is employed, where each record is assigned to exactly one block. Assume this method results in $b$ number of blocks.\n",
    "\n",
    "2. What is the smallest possible number of comparisons? What is the value of $b$ in this scenario?\n",
    "3. What is the largest possible number of comparisons? What is the value of $b$ in this scenario?\n",
    "4. What is the advantage with a large $b$? What is the advantage with a small $b$?\n",
    "\n",
    "In practice, a record is assigned to more than one block and records are not evenly allocated to blocks (for example, `alissa kilmartin` is assigned to the block with `surname_prefix==kilm` and `given_name_prefix==alis`).\n",
    "    \n",
    "5. How would this affect your answer to question 4?\n",
    "6. What is the advantage of records being assigned to multiple blocks?\n",
    "    \n",
    "We've seen how accuracy does not truly reflect the performance of a blocking algorithm. Instead, the confusion matrix can be used to evaluate the performance based on TP, TN, FP, FN.\n",
    "    \n",
    "7. It is desirable to minimise both FP and FN, but it may be diffcult for a blocking algorithm to minimise both simultaneously. Give an example application where minimising FP is more important than minimising FN. Give an example application where minimising FN is more important than minimising FP.\n",
    "       \n",
    "8. When matching sensitive string records, the data is often hashed into **bloom filters**. Explain how string matching is performed with bloom filters, and the pros and cons of using bloom filter-based comparison instead of exact string comparison."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer:\n",
    "1. maximum number of record matches is m, number of non-matches is m* (n-1)\n",
    "2. 2 blocks, block A contains all records from dataset A, block B contains all records from dataset B. Thus, there's no comparisons.\n",
    "3. m x n comparisons when b=1 (all records from dataset A & B in the same block)\n",
    "4. fast, inaccurate vs slow, accurate\n",
    "5. Dominant block size is the bottleneck for effciency. So, even if b is large, the blocking can still lead to ineffcient record linkage Small b tends to correspond to ineffciency; but it does not automatically guarantee accuracy. If matches are all of blocks, accuracy can still be low.\n",
    "6. To reduce the chance of matched pairs not allocated to a common block. Matched pairs may not have the same parts of the records in common, sometimes, a blocking key may miss a pair but captured by another blocking key.\n",
    "7. \n",
    "\n",
    "More important to minimise FP: e.g. Centrelink - might want to avoid sending out debt recovery letters to people who don't actually owe a debt \n",
    "\n",
    "Minimise FN: e.g. Medical screening tests - Often used to flag people who may have a particular disease for further tests. Don't want a FN to\n",
    "mean that people who actually do have the disease don't receive further tests and treatment. Also national security, don't want a FN to mean\n",
    "that a potential terrorist plot is not followed up on.\n",
    "\n",
    "In practice there's often a trade-off involving the resources available for\n",
    "further investigations. We are willing to have a number of false positives\n",
    "to ensure we're avoiding false negatives, but don't want so many false\n",
    "positives that we can't manually screen and investigate them all."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <u> Challenge question </u>\n",
    "\n",
    "The following section will introduce you to a Python implementation of bloom filters, using the `mmh3` library to implement the MurmurHash (MurmurHash3) hashing function. Note that you can use other hashing functions for bloom filters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mmh3\n",
    "from nltk.util import bigrams\n",
    "\n",
    "def bloom_filter(string1, string2, I, k):\n",
    "    \n",
    "    \"\"\"\n",
    "    Return two I-length bloom filters for string1 and string2 using k MMH3 hash functions\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create a string into a set of smaller strings (bigrams)\n",
    "    bigrams1 = [''.join(e) for e in bigrams(string1)]\n",
    "    bigrams2 = [''.join(e) for e in bigrams(string2)]\n",
    "    \n",
    "    # Initialize the bloom filters\n",
    "    bloomfilter1 = [0]*I\n",
    "    bloomfilter2 = [0]*I\n",
    "\n",
    "    for i in range(k):\n",
    "    # For each hashing function, apply it to each element of the bigram lists, and update the according index to 1\n",
    "        for w1 in bigrams1:\n",
    "            idx1 = mmh3.hash(w1, seed=i) % I # Question: Why use the modulo function here?\n",
    "            bloomfilter1[idx1] = 1\n",
    "        for w2 in bigrams2:\n",
    "            idx2 = mmh3.hash(w2, seed=i) % I\n",
    "            bloomfilter2[idx2] = 1\n",
    "\n",
    "    return bloomfilter1, bloomfilter2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bloom filter for _SMITH_: [0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1]\n",
      "Bloom filter for _SMYTH_: [0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "bloomfilter1, bloomfilter2 = bloom_filter('_SMITH_', '_SMYTH_', I = 30, k = 2)\n",
    "\n",
    "print('Bloom filter for _SMITH_:', bloomfilter1)\n",
    "print('Bloom filter for _SMYTH_:', bloomfilter2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a function `dice_sim_bloomfilter()` that uses the bloom filters returned by `bloom_filter()` to calculate the Dice similarity scores. Your function should return the following results:\n",
    "\n",
    "    print(dice_sim_bloomfilter('_SMITH_', '_SMYTH_', I = 30, k = 2))\n",
    "    0.6666666666666666\n",
    "    print(dice_sim_bloomfilter('_SMITH_', '_SMITH_', I = 30, k = 2))\n",
    "    1.0\n",
    "    print(dice_sim_bloomfilter('_SMITH_', '_SMOTH_', I = 30, k = 2))\n",
    "    0.7619047619047619\n",
    "    print(dice_sim_bloomfilter('_SMOTH_', '_SMYTH_', I = 30, k = 2))\n",
    "    0.6363636363636364"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice_sim_bloomfilter(string1, string2, I, k):\n",
    "    ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try running the function with different values of `I` and `k` to investigate the effect of these parameters on the similarity scores. What are the pros and cons of having high `I`? What are the pros and cons of having high `k`?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
