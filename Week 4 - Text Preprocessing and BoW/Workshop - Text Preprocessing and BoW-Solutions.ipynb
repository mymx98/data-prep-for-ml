{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case Study Background\n",
    "Following up from our previous work, \n",
    "after extracting relevant text information from the `review` and `address` columns, \n",
    "it's time to \"grade\" whether a review is positive or negative. \n",
    "This is called \"sentiment analysis\", and it's a widely studied problem in Natural Language Processing (NLP). \n",
    "To do this, we need to pre-process our text-based reviews, \n",
    "and transform them into a vector representation. \n",
    "Note that we won't be building the sentiment analyser, \n",
    "but will only practice the steps leading up to it. \n",
    "However, we do have a bonus for you at the end of this spreadsheet to have a taste of how sentiment analysis works!\n",
    "\n",
    "## Learning objectives\n",
    "- Understand the process and usage of various text preprocessing techniques: tokenisation, casefolding, noise & stopwords removal\n",
    "- Understand the differences between stemming and lemmatisation\n",
    "- Be aware of the waterfall effects of various choices in the preprocessing pipeline\n",
    "- Understand the idea of vector representation, sparse matrix, and Bag-of-Words\n",
    "- Know how to implement these preprocessing and vectorization steps using the `sklearn` and `nltk` libraries\n",
    "\n",
    "## Workshop Overview\n",
    "- Apply preprocessing steps to a single review parapgraph\n",
    "- Compare the differences in 2 alternative preprocessing pipelines at each step\n",
    "- Compare the differences in 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>address</th>\n",
       "      <th>state</th>\n",
       "      <th>captainmarvel</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Avengers: Endgame is dumb. Very dumb. It's a m...</td>\n",
       "      <td>39 Aaron Place Norwood VIC 5091</td>\n",
       "      <td>VIC</td>\n",
       "      <td>NO MATCH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What an unbelievable accomplishment to have sh...</td>\n",
       "      <td>461 Achernar Close Bondi Junction VIC 5125</td>\n",
       "      <td>VIC</td>\n",
       "      <td>NO MATCH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Disclosure: I'm NOT a Marvel superfan, but I'v...</td>\n",
       "      <td>24 Adair Street Bundaberg VIC 2127</td>\n",
       "      <td>VIC</td>\n",
       "      <td>capt. marvel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"Avengers: Endgame\" is about memories, nostalg...</td>\n",
       "      <td>51 Academy Close Floreat VIC 2680</td>\n",
       "      <td>VIC</td>\n",
       "      <td>\"captain marvel\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>It feels and watches like a seasin finale of a...</td>\n",
       "      <td>12 Abercorn Crescent Joondalup NSW 3055</td>\n",
       "      <td>NSW</td>\n",
       "      <td>NO MATCH</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  \\\n",
       "0  Avengers: Endgame is dumb. Very dumb. It's a m...   \n",
       "1  What an unbelievable accomplishment to have sh...   \n",
       "2  Disclosure: I'm NOT a Marvel superfan, but I'v...   \n",
       "3  \"Avengers: Endgame\" is about memories, nostalg...   \n",
       "4  It feels and watches like a seasin finale of a...   \n",
       "\n",
       "                                      address state     captainmarvel  \n",
       "0             39 Aaron Place Norwood VIC 5091   VIC          NO MATCH  \n",
       "1  461 Achernar Close Bondi Junction VIC 5125   VIC          NO MATCH  \n",
       "2          24 Adair Street Bundaberg VIC 2127   VIC      capt. marvel  \n",
       "3           51 Academy Close Floreat VIC 2680   VIC  \"captain marvel\"  \n",
       "4     12 Abercorn Crescent Joondalup NSW 3055   NSW          NO MATCH  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "data = pd.read_csv('nlp.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text pre-processing\n",
    "\n",
    "While there are many techniques you can apply at this phase, we will perform only a few steps in this tutorial. We will leave the remaining techniques as Challenge questions for you to perform in your own time. We will apply it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go ahead and watch the trailers for Avengers: Endgame, they won't give anything major away. It's amazing for a huge movie to be so self-aware of itself, as well as, the movie genres that they're overtly borrowing from. The minor characters or those not even in Avengers: Infinity War, step up and help set up huge sequences that are highly entertaining and actually answer questions. Avengers: Endgame acknowledges every aspect of the characters emotions in their previous MCU film's and succeeds in the most Meta way possible.Ant-Man is a major reason for this. It's no spoiler to say that he's in the film as he produces some of the biggest laughs from the trailer of him ringing the bell at the Avenger's front gate. It's Paul Rudd's wry jokes, quick timing and fish out of water facial expressions that really assist things.The pacing of Avengers: Endgame is amazing and not for the reasons you might think. It's brilliantly paced, but it throws the entire formula and how MCU films are done on their head. Battle, loss, battle, loss, humor and then victory, take those ingredients, shake well and you've got a loose description of some average superhero films.If Avengers: Infinity War left you feeling betrayed and bummed out, I feel you. Avengers: Endgame more than makes up for any ill feelings that it caused. I see why they played it the way they did, and doing so any other way would've cheapened the film, watered down the MCU and made it all disposable. It is also worth noting that the film balance lots of humor with the drama, all the while putting it in a nice bow with stones.What we're left with is a 10 year opus that spans more arcs than Lord of the Rings and encapsulates it all in a perfect manner. Granted, they will still make many MCU films, this is not their swan song. However, there is a victory lap feeling about Avengers: Endgame. It respects the fans, adds to the dramatic elements, answers questions that fans have wondered about and plants a couple new seeds to grow on.\n"
     ]
    }
   ],
   "source": [
    "# We'll apply the techniques on this sample review\n",
    "sample_review = data['review'][37]\n",
    "print(sample_review)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing technique 1: Casefolding\n",
    "\n",
    "<blockquote style=\"padding: 10px; background-color: #FFD392;\">\n",
    "\n",
    "## Discussion questions\n",
    "    \n",
    "1. What's the use of casefolding?\n",
    "2. In which situations should you **NOT** do casefolding?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "go ahead and watch the trailers for avengers: endgame, they won't give anything major away. it's amazing for a huge movie to be so self-aware of itself, as well as, the movie genres that they're overtly borrowing from. the minor characters or those not even in avengers: infinity war, step up and help set up huge sequences that are highly entertaining and actually answer questions. avengers: endgame acknowledges every aspect of the characters emotions in their previous mcu film's and succeeds in the most meta way possible.ant-man is a major reason for this. it's no spoiler to say that he's in the film as he produces some of the biggest laughs from the trailer of him ringing the bell at the avenger's front gate. it's paul rudd's wry jokes, quick timing and fish out of water facial expressions that really assist things.the pacing of avengers: endgame is amazing and not for the reasons you might think. it's brilliantly paced, but it throws the entire formula and how mcu films are done on their head. battle, loss, battle, loss, humor and then victory, take those ingredients, shake well and you've got a loose description of some average superhero films.if avengers: infinity war left you feeling betrayed and bummed out, i feel you. avengers: endgame more than makes up for any ill feelings that it caused. i see why they played it the way they did, and doing so any other way would've cheapened the film, watered down the mcu and made it all disposable. it is also worth noting that the film balance lots of humor with the drama, all the while putting it in a nice bow with stones.what we're left with is a 10 year opus that spans more arcs than lord of the rings and encapsulates it all in a perfect manner. granted, they will still make many mcu films, this is not their swan song. however, there is a victory lap feeling about avengers: endgame. it respects the fans, adds to the dramatic elements, answers questions that fans have wondered about and plants a couple new seeds to grow on.\n"
     ]
    }
   ],
   "source": [
    "lowercased = sample_review.lower()\n",
    "print(lowercased)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing technique 2: Noise (punctuation) removal\n",
    "\n",
    "<blockquote style=\"padding: 10px; background-color: #FFD392;\">\n",
    "\n",
    "## Exercise\n",
    "Write regex to remove all the punctuations and numbers. Make sure you take care of special cases of full-stops like `way possible.ant-man` so that it doesn't become `possibleantman`!. Store your output in a variable named `no_punct`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "go ahead and watch the trailers for avengers endgame they wont give anything major away its amazing for a huge movie to be so selfaware of itself as well as the movie genres that theyre overtly borrowing from the minor characters or those not even in avengers infinity war step up and help set up huge sequences that are highly entertaining and actually answer questions avengers endgame acknowledges every aspect of the characters emotions in their previous mcu films and succeeds in the most meta way possible antman is a major reason for this its no spoiler to say that hes in the film as he produces some of the biggest laughs from the trailer of him ringing the bell at the avengers front gate its paul rudds wry jokes quick timing and fish out of water facial expressions that really assist things the pacing of avengers endgame is amazing and not for the reasons you might think its brilliantly paced but it throws the entire formula and how mcu films are done on their head battle loss battle loss humor and then victory take those ingredients shake well and youve got a loose description of some average superhero films if avengers infinity war left you feeling betrayed and bummed out i feel you avengers endgame more than makes up for any ill feelings that it caused i see why they played it the way they did and doing so any other way wouldve cheapened the film watered down the mcu and made it all disposable it is also worth noting that the film balance lots of humor with the drama all the while putting it in a nice bow with stones what were left with is a  year opus that spans more arcs than lord of the rings and encapsulates it all in a perfect manner granted they will still make many mcu films this is not their swan song however there is a victory lap feeling about avengers endgame it respects the fans adds to the dramatic elements answers questions that fans have wondered about and plants a couple new seeds to grow on\n"
     ]
    }
   ],
   "source": [
    "# ANSWER HERE \n",
    "\n",
    "# SOLUTION\n",
    "\n",
    "special_full_stop = re.sub(r'\\.(?=\\w)','. ', lowercased)  # Add a space after fullstop without a space\n",
    "no_punct = re.sub(r'[^A-z\\s]', '', special_full_stop)  # Replace everything except characters and space\n",
    "print(no_punct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Follow-up question:** \n",
    "\n",
    "Discuss one disadvantage of this punctuation removal strategy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing technique 3: Tokenisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "# nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['go', 'ahead', 'and', 'watch', 'the', 'trailers', 'for', 'avengers', 'endgame', 'they', 'wont', 'give', 'anything', 'major', 'away', 'its', 'amazing', 'for', 'a', 'huge', 'movie', 'to', 'be', 'so', 'selfaware', 'of', 'itself', 'as', 'well', 'as', 'the', 'movie', 'genres', 'that', 'theyre', 'overtly', 'borrowing', 'from', 'the', 'minor', 'characters', 'or', 'those', 'not', 'even', 'in', 'avengers', 'infinity', 'war', 'step', 'up', 'and', 'help', 'set', 'up', 'huge', 'sequences', 'that', 'are', 'highly', 'entertaining', 'and', 'actually', 'answer', 'questions', 'avengers', 'endgame', 'acknowledges', 'every', 'aspect', 'of', 'the', 'characters', 'emotions', 'in', 'their', 'previous', 'mcu', 'films', 'and', 'succeeds', 'in', 'the', 'most', 'meta', 'way', 'possible', 'antman', 'is', 'a', 'major', 'reason', 'for', 'this', 'its', 'no', 'spoiler', 'to', 'say', 'that', 'hes', 'in', 'the', 'film', 'as', 'he', 'produces', 'some', 'of', 'the', 'biggest', 'laughs', 'from', 'the', 'trailer', 'of', 'him', 'ringing', 'the', 'bell', 'at', 'the', 'avengers', 'front', 'gate', 'its', 'paul', 'rudds', 'wry', 'jokes', 'quick', 'timing', 'and', 'fish', 'out', 'of', 'water', 'facial', 'expressions', 'that', 'really', 'assist', 'things', 'the', 'pacing', 'of', 'avengers', 'endgame', 'is', 'amazing', 'and', 'not', 'for', 'the', 'reasons', 'you', 'might', 'think', 'its', 'brilliantly', 'paced', 'but', 'it', 'throws', 'the', 'entire', 'formula', 'and', 'how', 'mcu', 'films', 'are', 'done', 'on', 'their', 'head', 'battle', 'loss', 'battle', 'loss', 'humor', 'and', 'then', 'victory', 'take', 'those', 'ingredients', 'shake', 'well', 'and', 'youve', 'got', 'a', 'loose', 'description', 'of', 'some', 'average', 'superhero', 'films', 'if', 'avengers', 'infinity', 'war', 'left', 'you', 'feeling', 'betrayed', 'and', 'bummed', 'out', 'i', 'feel', 'you', 'avengers', 'endgame', 'more', 'than', 'makes', 'up', 'for', 'any', 'ill', 'feelings', 'that', 'it', 'caused', 'i', 'see', 'why', 'they', 'played', 'it', 'the', 'way', 'they', 'did', 'and', 'doing', 'so', 'any', 'other', 'way', 'wouldve', 'cheapened', 'the', 'film', 'watered', 'down', 'the', 'mcu', 'and', 'made', 'it', 'all', 'disposable', 'it', 'is', 'also', 'worth', 'noting', 'that', 'the', 'film', 'balance', 'lots', 'of', 'humor', 'with', 'the', 'drama', 'all', 'the', 'while', 'putting', 'it', 'in', 'a', 'nice', 'bow', 'with', 'stones', 'what', 'were', 'left', 'with', 'is', 'a', 'year', 'opus', 'that', 'spans', 'more', 'arcs', 'than', 'lord', 'of', 'the', 'rings', 'and', 'encapsulates', 'it', 'all', 'in', 'a', 'perfect', 'manner', 'granted', 'they', 'will', 'still', 'make', 'many', 'mcu', 'films', 'this', 'is', 'not', 'their', 'swan', 'song', 'however', 'there', 'is', 'a', 'victory', 'lap', 'feeling', 'about', 'avengers', 'endgame', 'it', 'respects', 'the', 'fans', 'adds', 'to', 'the', 'dramatic', 'elements', 'answers', 'questions', 'that', 'fans', 'have', 'wondered', 'about', 'and', 'plants', 'a', 'couple', 'new', 'seeds', 'to', 'grow', 'on']\n"
     ]
    }
   ],
   "source": [
    "tokens = nltk.word_tokenize(no_punct)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<blockquote style=\"padding: 10px; background-color: #FFD392;\">\n",
    "\n",
    "## Exercise\n",
    "\n",
    "Another data scientist in your team suggested an alternative noise removal strategy. Compare your tokenised set of words with this alternative solution. Is there any difference? Which solution is better in this case?\n",
    "\n",
    "    special_full_stop = re.sub(r'\\.(?=\\w)','. ', lowercased)\n",
    "    punct2space = re.sub(r'[^A-z\\s]', ' ', special_full_stop)\n",
    "    no_punct_alt = re.sub(r'\\s+', ' ', punct2space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extra terms in tokens: ['selfaware', 'hes', 'antman', 'its', 'theyre', 'were', 'wouldve', 'youve', 'rudds', 'wont']\n",
      "Extra terms in tokens_alt: ['ve', 'won', 's', 'would', 't', 'rudd', 'aware', 'man', 'self', 'we', 'avenger', 'ant', 're']\n"
     ]
    }
   ],
   "source": [
    "# ANSWER HERE \n",
    "\n",
    "# SOLUTION\n",
    "\n",
    "special_full_stop = re.sub(r'\\.(?=\\w)','. ', lowercased)\n",
    "punct2space = re.sub(r'[^A-z\\s]', ' ', special_full_stop)\n",
    "no_punct_alt = re.sub(r'\\s+', ' ', punct2space)\n",
    "tokens_alt = nltk.word_tokenize(no_punct_alt)\n",
    "extra_terms_in_tokens = list(set(tokens) - set(tokens_alt))\n",
    "extra_terms_in_tokens_alt = list(set(tokens_alt)-set(tokens))\n",
    "print('Extra terms in tokens:', extra_terms_in_tokens)\n",
    "print('Extra terms in tokens_alt:',extra_terms_in_tokens_alt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing technique 4: Stopword removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a',\n",
       " 'about',\n",
       " 'above',\n",
       " 'after',\n",
       " 'again',\n",
       " 'against',\n",
       " 'ain',\n",
       " 'all',\n",
       " 'am',\n",
       " 'an',\n",
       " 'and',\n",
       " 'any',\n",
       " 'are',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'as',\n",
       " 'at',\n",
       " 'be',\n",
       " 'because',\n",
       " 'been',\n",
       " 'before',\n",
       " 'being',\n",
       " 'below',\n",
       " 'between',\n",
       " 'both',\n",
       " 'but',\n",
       " 'by',\n",
       " 'can',\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'd',\n",
       " 'did',\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'do',\n",
       " 'does',\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'doing',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'down',\n",
       " 'during',\n",
       " 'each',\n",
       " 'few',\n",
       " 'for',\n",
       " 'from',\n",
       " 'further',\n",
       " 'had',\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'has',\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'have',\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'having',\n",
       " 'he',\n",
       " 'her',\n",
       " 'here',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'him',\n",
       " 'himself',\n",
       " 'his',\n",
       " 'how',\n",
       " 'i',\n",
       " 'if',\n",
       " 'in',\n",
       " 'into',\n",
       " 'is',\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'just',\n",
       " 'll',\n",
       " 'm',\n",
       " 'ma',\n",
       " 'me',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'more',\n",
       " 'most',\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'my',\n",
       " 'myself',\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'now',\n",
       " 'o',\n",
       " 'of',\n",
       " 'off',\n",
       " 'on',\n",
       " 'once',\n",
       " 'only',\n",
       " 'or',\n",
       " 'other',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'out',\n",
       " 'over',\n",
       " 'own',\n",
       " 're',\n",
       " 's',\n",
       " 'same',\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'so',\n",
       " 'some',\n",
       " 'such',\n",
       " 't',\n",
       " 'than',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'the',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'them',\n",
       " 'themselves',\n",
       " 'then',\n",
       " 'there',\n",
       " 'these',\n",
       " 'they',\n",
       " 'this',\n",
       " 'those',\n",
       " 'through',\n",
       " 'to',\n",
       " 'too',\n",
       " 'under',\n",
       " 'until',\n",
       " 'up',\n",
       " 've',\n",
       " 'very',\n",
       " 'was',\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'we',\n",
       " 'were',\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'what',\n",
       " 'when',\n",
       " 'where',\n",
       " 'which',\n",
       " 'while',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'why',\n",
       " 'will',\n",
       " 'with',\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\",\n",
       " 'y',\n",
       " 'you',\n",
       " \"you'd\",\n",
       " \"you'll\",\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "# run the line to download it the first time:\n",
    "# nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stop_words # Check out the list of stopwords defined in NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original tokenized size: 356\n",
      "After removing stopwords: 188\n",
      "\n",
      "['go', 'ahead', 'watch', 'trailers', 'avengers', 'endgame', 'wont', 'give', 'anything', 'major', 'away', 'amazing', 'huge', 'movie', 'selfaware', 'well', 'movie', 'genres', 'theyre', 'overtly', 'borrowing', 'minor', 'characters', 'even', 'avengers', 'infinity', 'war', 'step', 'help', 'set', 'huge', 'sequences', 'highly', 'entertaining', 'actually', 'answer', 'questions', 'avengers', 'endgame', 'acknowledges', 'every', 'aspect', 'characters', 'emotions', 'previous', 'mcu', 'films', 'succeeds', 'meta', 'way', 'possible', 'antman', 'major', 'reason', 'spoiler', 'say', 'hes', 'film', 'produces', 'biggest', 'laughs', 'trailer', 'ringing', 'bell', 'avengers', 'front', 'gate', 'paul', 'rudds', 'wry', 'jokes', 'quick', 'timing', 'fish', 'water', 'facial', 'expressions', 'really', 'assist', 'things', 'pacing', 'avengers', 'endgame', 'amazing', 'reasons', 'might', 'think', 'brilliantly', 'paced', 'throws', 'entire', 'formula', 'mcu', 'films', 'done', 'head', 'battle', 'loss', 'battle', 'loss', 'humor', 'victory', 'take', 'ingredients', 'shake', 'well', 'youve', 'got', 'loose', 'description', 'average', 'superhero', 'films', 'avengers', 'infinity', 'war', 'left', 'feeling', 'betrayed', 'bummed', 'feel', 'avengers', 'endgame', 'makes', 'ill', 'feelings', 'caused', 'see', 'played', 'way', 'way', 'wouldve', 'cheapened', 'film', 'watered', 'mcu', 'made', 'disposable', 'also', 'worth', 'noting', 'film', 'balance', 'lots', 'humor', 'drama', 'putting', 'nice', 'bow', 'stones', 'left', 'year', 'opus', 'spans', 'arcs', 'lord', 'rings', 'encapsulates', 'perfect', 'manner', 'granted', 'still', 'make', 'many', 'mcu', 'films', 'swan', 'song', 'however', 'victory', 'lap', 'feeling', 'avengers', 'endgame', 'respects', 'fans', 'adds', 'dramatic', 'elements', 'answers', 'questions', 'fans', 'wondered', 'plants', 'couple', 'new', 'seeds', 'grow']\n"
     ]
    }
   ],
   "source": [
    "no_stopwords = [w for w in tokens if not w in stop_words]\n",
    "print(\"Original tokenized size:\", len(tokens))\n",
    "print(\"After removing stopwords:\", len(no_stopwords))\n",
    "print()\n",
    "print(no_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After removing stopwords (different noise removal strategy): 186\n",
      "\n",
      "punct-to-null results in words like:\n",
      "['wont', 'selfaware', 'theyre', 'antman', 'hes', 'rudds', 'youve', 'wouldve']\n",
      "\n",
      "punct-to-space results in words like:\n",
      "['self', 'aware', 'ant', 'man', 'avenger', 'rudd', 'would']\n"
     ]
    }
   ],
   "source": [
    "# Comparison with the alternative strategy\n",
    "no_stopwords_alt = [w for w in tokens_alt if not w in stop_words]\n",
    "\n",
    "print(\"After removing stopwords (different noise removal strategy):\", len(no_stopwords_alt))\n",
    "print()\n",
    "\n",
    "print('punct-to-null results in words like:')\n",
    "print([w for w in no_stopwords if w not in no_stopwords_alt])\n",
    "print()\n",
    "print('punct-to-space results in words like:')\n",
    "print([w for w in no_stopwords_alt if w not in no_stopwords])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing technique 5a: Stemming "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "# Instantiating the stemmer algorithm\n",
    "porterStemmer = PorterStemmer()\n",
    "# Stem the no stopword lists\n",
    "stemmed = [porterStemmer.stem(w) for w in no_stopwords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before: go | Stemmed: go\n",
      "Before: ahead | Stemmed: ahead\n",
      "Before: watch | Stemmed: watch\n",
      "Before: trailers | Stemmed: trailer\n",
      "Before: avengers | Stemmed: aveng\n",
      "Before: endgame | Stemmed: endgam\n",
      "Before: wont | Stemmed: wont\n",
      "Before: give | Stemmed: give\n",
      "Before: anything | Stemmed: anyth\n",
      "Before: major | Stemmed: major\n",
      "Before: away | Stemmed: away\n",
      "Before: amazing | Stemmed: amaz\n",
      "Before: huge | Stemmed: huge\n",
      "Before: movie | Stemmed: movi\n",
      "Before: selfaware | Stemmed: selfawar\n",
      "Before: well | Stemmed: well\n",
      "Before: movie | Stemmed: movi\n",
      "Before: genres | Stemmed: genr\n",
      "Before: theyre | Stemmed: theyr\n",
      "Before: overtly | Stemmed: overtli\n",
      "Before: borrowing | Stemmed: borrow\n",
      "Before: minor | Stemmed: minor\n",
      "Before: characters | Stemmed: charact\n",
      "Before: even | Stemmed: even\n",
      "Before: avengers | Stemmed: aveng\n",
      "Before: infinity | Stemmed: infin\n",
      "Before: war | Stemmed: war\n",
      "Before: step | Stemmed: step\n",
      "Before: help | Stemmed: help\n",
      "Before: set | Stemmed: set\n",
      "Before: huge | Stemmed: huge\n",
      "Before: sequences | Stemmed: sequenc\n",
      "Before: highly | Stemmed: highli\n",
      "Before: entertaining | Stemmed: entertain\n",
      "Before: actually | Stemmed: actual\n",
      "Before: answer | Stemmed: answer\n",
      "Before: questions | Stemmed: question\n",
      "Before: avengers | Stemmed: aveng\n",
      "Before: endgame | Stemmed: endgam\n",
      "Before: acknowledges | Stemmed: acknowledg\n",
      "Before: every | Stemmed: everi\n",
      "Before: aspect | Stemmed: aspect\n",
      "Before: characters | Stemmed: charact\n",
      "Before: emotions | Stemmed: emot\n",
      "Before: previous | Stemmed: previou\n",
      "Before: mcu | Stemmed: mcu\n",
      "Before: films | Stemmed: film\n",
      "Before: succeeds | Stemmed: succe\n",
      "Before: meta | Stemmed: meta\n",
      "Before: way | Stemmed: way\n",
      "Before: possible | Stemmed: possibl\n",
      "Before: antman | Stemmed: antman\n",
      "Before: major | Stemmed: major\n",
      "Before: reason | Stemmed: reason\n",
      "Before: spoiler | Stemmed: spoiler\n",
      "Before: say | Stemmed: say\n",
      "Before: hes | Stemmed: he\n",
      "Before: film | Stemmed: film\n",
      "Before: produces | Stemmed: produc\n",
      "Before: biggest | Stemmed: biggest\n",
      "Before: laughs | Stemmed: laugh\n",
      "Before: trailer | Stemmed: trailer\n",
      "Before: ringing | Stemmed: ring\n",
      "Before: bell | Stemmed: bell\n",
      "Before: avengers | Stemmed: aveng\n",
      "Before: front | Stemmed: front\n",
      "Before: gate | Stemmed: gate\n",
      "Before: paul | Stemmed: paul\n",
      "Before: rudds | Stemmed: rudd\n",
      "Before: wry | Stemmed: wri\n",
      "Before: jokes | Stemmed: joke\n",
      "Before: quick | Stemmed: quick\n",
      "Before: timing | Stemmed: time\n",
      "Before: fish | Stemmed: fish\n",
      "Before: water | Stemmed: water\n",
      "Before: facial | Stemmed: facial\n",
      "Before: expressions | Stemmed: express\n",
      "Before: really | Stemmed: realli\n",
      "Before: assist | Stemmed: assist\n",
      "Before: things | Stemmed: thing\n",
      "Before: pacing | Stemmed: pace\n",
      "Before: avengers | Stemmed: aveng\n",
      "Before: endgame | Stemmed: endgam\n",
      "Before: amazing | Stemmed: amaz\n",
      "Before: reasons | Stemmed: reason\n",
      "Before: might | Stemmed: might\n",
      "Before: think | Stemmed: think\n",
      "Before: brilliantly | Stemmed: brilliantli\n",
      "Before: paced | Stemmed: pace\n",
      "Before: throws | Stemmed: throw\n",
      "Before: entire | Stemmed: entir\n",
      "Before: formula | Stemmed: formula\n",
      "Before: mcu | Stemmed: mcu\n",
      "Before: films | Stemmed: film\n",
      "Before: done | Stemmed: done\n",
      "Before: head | Stemmed: head\n",
      "Before: battle | Stemmed: battl\n",
      "Before: loss | Stemmed: loss\n",
      "Before: battle | Stemmed: battl\n",
      "Before: loss | Stemmed: loss\n",
      "Before: humor | Stemmed: humor\n",
      "Before: victory | Stemmed: victori\n",
      "Before: take | Stemmed: take\n",
      "Before: ingredients | Stemmed: ingredi\n",
      "Before: shake | Stemmed: shake\n",
      "Before: well | Stemmed: well\n",
      "Before: youve | Stemmed: youv\n",
      "Before: got | Stemmed: got\n",
      "Before: loose | Stemmed: loos\n",
      "Before: description | Stemmed: descript\n",
      "Before: average | Stemmed: averag\n",
      "Before: superhero | Stemmed: superhero\n",
      "Before: films | Stemmed: film\n",
      "Before: avengers | Stemmed: aveng\n",
      "Before: infinity | Stemmed: infin\n",
      "Before: war | Stemmed: war\n",
      "Before: left | Stemmed: left\n",
      "Before: feeling | Stemmed: feel\n",
      "Before: betrayed | Stemmed: betray\n",
      "Before: bummed | Stemmed: bum\n",
      "Before: feel | Stemmed: feel\n",
      "Before: avengers | Stemmed: aveng\n",
      "Before: endgame | Stemmed: endgam\n",
      "Before: makes | Stemmed: make\n",
      "Before: ill | Stemmed: ill\n",
      "Before: feelings | Stemmed: feel\n",
      "Before: caused | Stemmed: caus\n",
      "Before: see | Stemmed: see\n",
      "Before: played | Stemmed: play\n",
      "Before: way | Stemmed: way\n",
      "Before: way | Stemmed: way\n",
      "Before: wouldve | Stemmed: wouldv\n",
      "Before: cheapened | Stemmed: cheapen\n",
      "Before: film | Stemmed: film\n",
      "Before: watered | Stemmed: water\n",
      "Before: mcu | Stemmed: mcu\n",
      "Before: made | Stemmed: made\n",
      "Before: disposable | Stemmed: dispos\n",
      "Before: also | Stemmed: also\n",
      "Before: worth | Stemmed: worth\n",
      "Before: noting | Stemmed: note\n",
      "Before: film | Stemmed: film\n",
      "Before: balance | Stemmed: balanc\n",
      "Before: lots | Stemmed: lot\n",
      "Before: humor | Stemmed: humor\n",
      "Before: drama | Stemmed: drama\n",
      "Before: putting | Stemmed: put\n",
      "Before: nice | Stemmed: nice\n",
      "Before: bow | Stemmed: bow\n",
      "Before: stones | Stemmed: stone\n",
      "Before: left | Stemmed: left\n",
      "Before: year | Stemmed: year\n",
      "Before: opus | Stemmed: opu\n",
      "Before: spans | Stemmed: span\n",
      "Before: arcs | Stemmed: arc\n",
      "Before: lord | Stemmed: lord\n",
      "Before: rings | Stemmed: ring\n",
      "Before: encapsulates | Stemmed: encapsul\n",
      "Before: perfect | Stemmed: perfect\n",
      "Before: manner | Stemmed: manner\n",
      "Before: granted | Stemmed: grant\n",
      "Before: still | Stemmed: still\n",
      "Before: make | Stemmed: make\n",
      "Before: many | Stemmed: mani\n",
      "Before: mcu | Stemmed: mcu\n",
      "Before: films | Stemmed: film\n",
      "Before: swan | Stemmed: swan\n",
      "Before: song | Stemmed: song\n",
      "Before: however | Stemmed: howev\n",
      "Before: victory | Stemmed: victori\n",
      "Before: lap | Stemmed: lap\n",
      "Before: feeling | Stemmed: feel\n",
      "Before: avengers | Stemmed: aveng\n",
      "Before: endgame | Stemmed: endgam\n",
      "Before: respects | Stemmed: respect\n",
      "Before: fans | Stemmed: fan\n",
      "Before: adds | Stemmed: add\n",
      "Before: dramatic | Stemmed: dramat\n",
      "Before: elements | Stemmed: element\n",
      "Before: answers | Stemmed: answer\n",
      "Before: questions | Stemmed: question\n",
      "Before: fans | Stemmed: fan\n",
      "Before: wondered | Stemmed: wonder\n",
      "Before: plants | Stemmed: plant\n",
      "Before: couple | Stemmed: coupl\n",
      "Before: new | Stemmed: new\n",
      "Before: seeds | Stemmed: seed\n",
      "Before: grow | Stemmed: grow\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(stemmed)):\n",
    "    print(\"Before:\", no_stopwords[i], \"| Stemmed:\", stemmed[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing technique 5b: Lemmatisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first time, run the line:\n",
    "# nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "# Instantiating the lemmatiser algorithm\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "# Lemmatise the no stopword lists\n",
    "lemmatized = [lemmatizer.lemmatize(w) for w in no_stopwords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatized: go | Stemmed: go\n",
      "Lemmatized: ahead | Stemmed: ahead\n",
      "Lemmatized: watch | Stemmed: watch\n",
      "Lemmatized: trailer | Stemmed: trailer\n",
      "Lemmatized: avenger | Stemmed: aveng\n",
      "Lemmatized: endgame | Stemmed: endgam\n",
      "Lemmatized: wont | Stemmed: wont\n",
      "Lemmatized: give | Stemmed: give\n",
      "Lemmatized: anything | Stemmed: anyth\n",
      "Lemmatized: major | Stemmed: major\n",
      "Lemmatized: away | Stemmed: away\n",
      "Lemmatized: amazing | Stemmed: amaz\n",
      "Lemmatized: huge | Stemmed: huge\n",
      "Lemmatized: movie | Stemmed: movi\n",
      "Lemmatized: selfaware | Stemmed: selfawar\n",
      "Lemmatized: well | Stemmed: well\n",
      "Lemmatized: movie | Stemmed: movi\n",
      "Lemmatized: genre | Stemmed: genr\n",
      "Lemmatized: theyre | Stemmed: theyr\n",
      "Lemmatized: overtly | Stemmed: overtli\n",
      "Lemmatized: borrowing | Stemmed: borrow\n",
      "Lemmatized: minor | Stemmed: minor\n",
      "Lemmatized: character | Stemmed: charact\n",
      "Lemmatized: even | Stemmed: even\n",
      "Lemmatized: avenger | Stemmed: aveng\n",
      "Lemmatized: infinity | Stemmed: infin\n",
      "Lemmatized: war | Stemmed: war\n",
      "Lemmatized: step | Stemmed: step\n",
      "Lemmatized: help | Stemmed: help\n",
      "Lemmatized: set | Stemmed: set\n",
      "Lemmatized: huge | Stemmed: huge\n",
      "Lemmatized: sequence | Stemmed: sequenc\n",
      "Lemmatized: highly | Stemmed: highli\n",
      "Lemmatized: entertaining | Stemmed: entertain\n",
      "Lemmatized: actually | Stemmed: actual\n",
      "Lemmatized: answer | Stemmed: answer\n",
      "Lemmatized: question | Stemmed: question\n",
      "Lemmatized: avenger | Stemmed: aveng\n",
      "Lemmatized: endgame | Stemmed: endgam\n",
      "Lemmatized: acknowledges | Stemmed: acknowledg\n",
      "Lemmatized: every | Stemmed: everi\n",
      "Lemmatized: aspect | Stemmed: aspect\n",
      "Lemmatized: character | Stemmed: charact\n",
      "Lemmatized: emotion | Stemmed: emot\n",
      "Lemmatized: previous | Stemmed: previou\n",
      "Lemmatized: mcu | Stemmed: mcu\n",
      "Lemmatized: film | Stemmed: film\n",
      "Lemmatized: succeeds | Stemmed: succe\n",
      "Lemmatized: meta | Stemmed: meta\n",
      "Lemmatized: way | Stemmed: way\n",
      "Lemmatized: possible | Stemmed: possibl\n",
      "Lemmatized: antman | Stemmed: antman\n",
      "Lemmatized: major | Stemmed: major\n",
      "Lemmatized: reason | Stemmed: reason\n",
      "Lemmatized: spoiler | Stemmed: spoiler\n",
      "Lemmatized: say | Stemmed: say\n",
      "Lemmatized: he | Stemmed: he\n",
      "Lemmatized: film | Stemmed: film\n",
      "Lemmatized: produce | Stemmed: produc\n",
      "Lemmatized: biggest | Stemmed: biggest\n",
      "Lemmatized: laugh | Stemmed: laugh\n",
      "Lemmatized: trailer | Stemmed: trailer\n",
      "Lemmatized: ringing | Stemmed: ring\n",
      "Lemmatized: bell | Stemmed: bell\n",
      "Lemmatized: avenger | Stemmed: aveng\n",
      "Lemmatized: front | Stemmed: front\n",
      "Lemmatized: gate | Stemmed: gate\n",
      "Lemmatized: paul | Stemmed: paul\n",
      "Lemmatized: rudd | Stemmed: rudd\n",
      "Lemmatized: wry | Stemmed: wri\n",
      "Lemmatized: joke | Stemmed: joke\n",
      "Lemmatized: quick | Stemmed: quick\n",
      "Lemmatized: timing | Stemmed: time\n",
      "Lemmatized: fish | Stemmed: fish\n",
      "Lemmatized: water | Stemmed: water\n",
      "Lemmatized: facial | Stemmed: facial\n",
      "Lemmatized: expression | Stemmed: express\n",
      "Lemmatized: really | Stemmed: realli\n",
      "Lemmatized: assist | Stemmed: assist\n",
      "Lemmatized: thing | Stemmed: thing\n",
      "Lemmatized: pacing | Stemmed: pace\n",
      "Lemmatized: avenger | Stemmed: aveng\n",
      "Lemmatized: endgame | Stemmed: endgam\n",
      "Lemmatized: amazing | Stemmed: amaz\n",
      "Lemmatized: reason | Stemmed: reason\n",
      "Lemmatized: might | Stemmed: might\n",
      "Lemmatized: think | Stemmed: think\n",
      "Lemmatized: brilliantly | Stemmed: brilliantli\n",
      "Lemmatized: paced | Stemmed: pace\n",
      "Lemmatized: throw | Stemmed: throw\n",
      "Lemmatized: entire | Stemmed: entir\n",
      "Lemmatized: formula | Stemmed: formula\n",
      "Lemmatized: mcu | Stemmed: mcu\n",
      "Lemmatized: film | Stemmed: film\n",
      "Lemmatized: done | Stemmed: done\n",
      "Lemmatized: head | Stemmed: head\n",
      "Lemmatized: battle | Stemmed: battl\n",
      "Lemmatized: loss | Stemmed: loss\n",
      "Lemmatized: battle | Stemmed: battl\n",
      "Lemmatized: loss | Stemmed: loss\n",
      "Lemmatized: humor | Stemmed: humor\n",
      "Lemmatized: victory | Stemmed: victori\n",
      "Lemmatized: take | Stemmed: take\n",
      "Lemmatized: ingredient | Stemmed: ingredi\n",
      "Lemmatized: shake | Stemmed: shake\n",
      "Lemmatized: well | Stemmed: well\n",
      "Lemmatized: youve | Stemmed: youv\n",
      "Lemmatized: got | Stemmed: got\n",
      "Lemmatized: loose | Stemmed: loos\n",
      "Lemmatized: description | Stemmed: descript\n",
      "Lemmatized: average | Stemmed: averag\n",
      "Lemmatized: superhero | Stemmed: superhero\n",
      "Lemmatized: film | Stemmed: film\n",
      "Lemmatized: avenger | Stemmed: aveng\n",
      "Lemmatized: infinity | Stemmed: infin\n",
      "Lemmatized: war | Stemmed: war\n",
      "Lemmatized: left | Stemmed: left\n",
      "Lemmatized: feeling | Stemmed: feel\n",
      "Lemmatized: betrayed | Stemmed: betray\n",
      "Lemmatized: bummed | Stemmed: bum\n",
      "Lemmatized: feel | Stemmed: feel\n",
      "Lemmatized: avenger | Stemmed: aveng\n",
      "Lemmatized: endgame | Stemmed: endgam\n",
      "Lemmatized: make | Stemmed: make\n",
      "Lemmatized: ill | Stemmed: ill\n",
      "Lemmatized: feeling | Stemmed: feel\n",
      "Lemmatized: caused | Stemmed: caus\n",
      "Lemmatized: see | Stemmed: see\n",
      "Lemmatized: played | Stemmed: play\n",
      "Lemmatized: way | Stemmed: way\n",
      "Lemmatized: way | Stemmed: way\n",
      "Lemmatized: wouldve | Stemmed: wouldv\n",
      "Lemmatized: cheapened | Stemmed: cheapen\n",
      "Lemmatized: film | Stemmed: film\n",
      "Lemmatized: watered | Stemmed: water\n",
      "Lemmatized: mcu | Stemmed: mcu\n",
      "Lemmatized: made | Stemmed: made\n",
      "Lemmatized: disposable | Stemmed: dispos\n",
      "Lemmatized: also | Stemmed: also\n",
      "Lemmatized: worth | Stemmed: worth\n",
      "Lemmatized: noting | Stemmed: note\n",
      "Lemmatized: film | Stemmed: film\n",
      "Lemmatized: balance | Stemmed: balanc\n",
      "Lemmatized: lot | Stemmed: lot\n",
      "Lemmatized: humor | Stemmed: humor\n",
      "Lemmatized: drama | Stemmed: drama\n",
      "Lemmatized: putting | Stemmed: put\n",
      "Lemmatized: nice | Stemmed: nice\n",
      "Lemmatized: bow | Stemmed: bow\n",
      "Lemmatized: stone | Stemmed: stone\n",
      "Lemmatized: left | Stemmed: left\n",
      "Lemmatized: year | Stemmed: year\n",
      "Lemmatized: opus | Stemmed: opu\n",
      "Lemmatized: span | Stemmed: span\n",
      "Lemmatized: arc | Stemmed: arc\n",
      "Lemmatized: lord | Stemmed: lord\n",
      "Lemmatized: ring | Stemmed: ring\n",
      "Lemmatized: encapsulates | Stemmed: encapsul\n",
      "Lemmatized: perfect | Stemmed: perfect\n",
      "Lemmatized: manner | Stemmed: manner\n",
      "Lemmatized: granted | Stemmed: grant\n",
      "Lemmatized: still | Stemmed: still\n",
      "Lemmatized: make | Stemmed: make\n",
      "Lemmatized: many | Stemmed: mani\n",
      "Lemmatized: mcu | Stemmed: mcu\n",
      "Lemmatized: film | Stemmed: film\n",
      "Lemmatized: swan | Stemmed: swan\n",
      "Lemmatized: song | Stemmed: song\n",
      "Lemmatized: however | Stemmed: howev\n",
      "Lemmatized: victory | Stemmed: victori\n",
      "Lemmatized: lap | Stemmed: lap\n",
      "Lemmatized: feeling | Stemmed: feel\n",
      "Lemmatized: avenger | Stemmed: aveng\n",
      "Lemmatized: endgame | Stemmed: endgam\n",
      "Lemmatized: respect | Stemmed: respect\n",
      "Lemmatized: fan | Stemmed: fan\n",
      "Lemmatized: add | Stemmed: add\n",
      "Lemmatized: dramatic | Stemmed: dramat\n",
      "Lemmatized: element | Stemmed: element\n",
      "Lemmatized: answer | Stemmed: answer\n",
      "Lemmatized: question | Stemmed: question\n",
      "Lemmatized: fan | Stemmed: fan\n",
      "Lemmatized: wondered | Stemmed: wonder\n",
      "Lemmatized: plant | Stemmed: plant\n",
      "Lemmatized: couple | Stemmed: coupl\n",
      "Lemmatized: new | Stemmed: new\n",
      "Lemmatized: seed | Stemmed: seed\n",
      "Lemmatized: grow | Stemmed: grow\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(lemmatized)):\n",
    "    print(\"Lemmatized:\", lemmatized[i], \"| Stemmed:\", stemmed[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<blockquote style=\"padding: 10px; background-color: #ebf5fb;\">\n",
    "\n",
    "    \n",
    "## Discussion question\n",
    "1. What's the difference between lemmatisation and stemming?\n",
    "2. When would we prefer stemming over lemmatising? And when would we prefer lemmatising over stemming?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag Of Words Vector Representation\n",
    "\n",
    "<blockquote style=\"padding: 10px; background-color: #ebf5fb;\">\n",
    "\n",
    "## Class Discussion Questions\n",
    "    \n",
    "1. What is text vector representation? \n",
    "2. What is a corpus? \n",
    "3. What is BoW?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In `sklearn`, we will use the in-built function `CountVectorizer` to encode `str` type text data into vector representation. The documentation for this function can be found here: https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<338x5175 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 30388 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "## Note that by default CountVectorizer only considers alphanumeric patterns of at least 2 characters to be a word.  \n",
    "## You can alter this behavior using the token_pattern parameter.\n",
    "vectorizer = CountVectorizer()\n",
    "bow = vectorizer.fit_transform(data['review'])\n",
    "bow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Follow-up question:** Why is it called a sparse matrix?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5175\n",
      "['000' '06' '10' '100' '1000' '10s' '11' '116' '12' '13' '14' '15' '180'\n",
      " '1970' '1987' '1990' '1st' '20' '200' '2008' '2010s' '2012' '2014' '2019'\n",
      " '21' '21st' '22' '22nd' '23' '250' '2d' '2hr' '2hrs' '2nd' '30' '300'\n",
      " '3000' '30h' '3d' '3h2' '3rd' '3seeing' '40' '45' '45mins' '47' '48'\n",
      " '4th' '50' '56' '5h' '60' '602' '70s' '75' '80s' '8th' '90' 'aaa'\n",
      " 'abandon' 'abandoned' 'abandoning' 'abandons' 'abilities' 'ability'\n",
      " 'able' 'abominably' 'about' 'above' 'abrams' 'abruptly' 'absent'\n",
      " 'absolute' 'absolutely' 'absorbed' 'absurd' 'absurdly' 'abundance'\n",
      " 'abysmal' 'abyss' 'accept' 'accepted' 'accident' 'accidentally'\n",
      " 'accompanied' 'accomplish' 'accomplished' 'accomplishment' 'according'\n",
      " 'account' 'accounts' 'accurate' 'accuser' 'achieve' 'achieved'\n",
      " 'achievement' 'achievements' 'achieving' 'acknowledge' 'acknowledges']\n"
     ]
    }
   ],
   "source": [
    "corpus = vectorizer.get_feature_names_out()\n",
    "print(len(corpus)) # There are 5175 interesting \"words\" after the preprocessing all 338 reviews!\n",
    "print(corpus[:100]) # Print the first 100 words in the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>5165</th>\n",
       "      <th>5166</th>\n",
       "      <th>5167</th>\n",
       "      <th>5168</th>\n",
       "      <th>5169</th>\n",
       "      <th>5170</th>\n",
       "      <th>5171</th>\n",
       "      <th>5172</th>\n",
       "      <th>5173</th>\n",
       "      <th>5174</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>333</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>334</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>335</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>336</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>337</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>338 rows  5175 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     0     1     2     3     4     5     6     7     8     9     ...  5165  \\\n",
       "0       0     0     0     0     0     0     0     0     0     0  ...     0   \n",
       "1       0     0     0     0     0     0     0     0     0     0  ...     0   \n",
       "2       0     0     0     0     0     0     0     0     0     0  ...     0   \n",
       "3       0     0     1     0     0     0     0     0     0     0  ...     0   \n",
       "4       0     0     0     0     0     0     0     0     0     0  ...     0   \n",
       "..    ...   ...   ...   ...   ...   ...   ...   ...   ...   ...  ...   ...   \n",
       "333     0     0     0     0     0     0     0     0     0     0  ...     0   \n",
       "334     0     0     0     0     0     0     0     0     0     0  ...     0   \n",
       "335     0     0     0     0     0     0     0     0     0     0  ...     0   \n",
       "336     0     0     0     0     0     0     0     0     0     0  ...     0   \n",
       "337     0     0     0     0     0     0     0     0     0     0  ...     0   \n",
       "\n",
       "     5166  5167  5168  5169  5170  5171  5172  5173  5174  \n",
       "0       0     0     0     0     0     0     0     0     0  \n",
       "1       0     0     0     0     0     0     0     0     0  \n",
       "2       0     0     0     0     0     0     0     0     0  \n",
       "3       0     0     0     0     0     0     0     0     0  \n",
       "4       0     0     0     0     0     0     0     0     0  \n",
       "..    ...   ...   ...   ...   ...   ...   ...   ...   ...  \n",
       "333     0     0     0     0     0     0     0     0     0  \n",
       "334     0     0     0     0     0     0     0     0     0  \n",
       "335     0     0     0     0     0     0     0     0     0  \n",
       "336     0     0     0     0     0     0     0     0     0  \n",
       "337     0     0     0     0     0     0     0     0     0  \n",
       "\n",
       "[338 rows x 5175 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print the BoW matrix\n",
    "pd.DataFrame(bow.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Follow-up question:** What does the '1' in row 3, column 2 mean?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Putting everything together: From raw text to cleaned BoW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<338x3651 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 20190 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def text_preprocess(doc, stop_words, stemmer):\n",
    "    doc = doc.lower()\n",
    "    doc = re.sub(r'\\.(?=\\w)','. ', doc)  # Add a space after fullstop without a space\n",
    "    doc = re.sub(r'[^A-z\\s]', ' ', doc)\n",
    "    doc = re.sub(r'\\s+', ' ', doc) \n",
    "    tokens = nltk.word_tokenize(doc)\n",
    "    tokens = [w for w in tokens if not w in stop_words]\n",
    "    stemmed = [stemmer.stem(w) for w in tokens]\n",
    "    return ' '.join(stemmed)\n",
    "    \n",
    "# Create input parameters for text_process\n",
    "stop_words = set(stopwords.words('english'))\n",
    "porterStemmer = PorterStemmer()\n",
    "reviews = data['review']\n",
    "\n",
    "# Loop through each review, preprocess, and append the output to cleaned_reviews\n",
    "cleaned_reviews = []\n",
    "for i, review in enumerate(reviews):\n",
    "    cleaned_reviews.append(text_preprocess(review, stop_words, porterStemmer))\n",
    "\n",
    "# BoW encoding    \n",
    "vectorizer1 = CountVectorizer()\n",
    "bow_cleaned = vectorizer1.fit_transform(cleaned_reviews)\n",
    "bow_cleaned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<blockquote style=\"padding: 10px; background-color: #ebf5fb;\">\n",
    "\n",
    "## Class Discussion Question\n",
    "Identify the preprocessing steps in the previous code block."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, unless the pre-processing steps required are very specific, NLTK has built-in pre-processors for Vectorizers, and we can utilise them easily. Check the documentation at https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html to find out what are the preprocessing steps built into (supported by) `CountVectorizer` function?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<338x3667 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 20219 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Alternative approach using CountVectorizer()\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "def pre_process(doc):\n",
    "    doc_new = doc.lower()\n",
    "    doc_new = re.sub(r'\\.(?=\\w)','. ', doc_new)  # Add a space after fullstop without a space\n",
    "    doc_new = re.sub(r'[^A-z\\s]', ' ', doc_new)  # remove punct\n",
    "    doc_new = re.sub(r'\\s+', ' ', doc_new)  # collapse multiple spaces.\n",
    "    return doc_new\n",
    "\n",
    "def tokeniser(doc):\n",
    "    tokens = nltk.word_tokenize(doc)\n",
    "    stop_words = set(stopwords.words('english'))    \n",
    "    tokens = [w for w in tokens if not w in stop_words]\n",
    "    porterStemmer = PorterStemmer()\n",
    "    stemmed = [porterStemmer.stem(w) for w in tokens]\n",
    "    return stemmed\n",
    "    \n",
    "    \n",
    "vectorizer2 = CountVectorizer(preprocessor=pre_process, tokenizer=tokeniser, analyzer = 'word')\n",
    "bow2 = vectorizer2.fit_transform(data['review'])\n",
    "bow2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Follow-up question:** Note the difference in the feature space of the output sparse matrix: 3651 vs 3667 (# of dimensions). Can you explain why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "specific to features1 (not in feature2):\n",
      "[]\n",
      "\n",
      "specific to features2 (not in feature1):\n",
      "['[', ']', 'b', 'c', 'e', 'f', 'g', 'h', 'l', 'n', 'p', 'q', 'u', 'w', 'x', 'z']\n",
      "\n",
      "specific to features_raw (not in feature2), selecting the first 100 features:\n",
      "['000', '06', '10', '100', '1000', '10s', '11', '116', '12', '13', '14', '15', '180', '1970', '1987', '1990', '1st', '20', '200', '2008', '2010s', '2012', '2014', '2019', '21', '21st', '22', '22nd', '23', '250', '2d', '2hr', '2hrs', '2nd', '30', '300', '3000', '30h', '3d', '3h2', '3rd', '3seeing', '40', '45', '45mins', '47', '48', '4th', '50', '56', '5h', '60', '602', '70s', '75', '80s', '8th', '90', 'abandoned', 'abandoning', 'abandons', 'abilities', 'ability', 'able', 'abominably', 'about', 'above', 'abrams', 'abruptly', 'absolute', 'absolutely', 'absorbed', 'absurdly', 'abundance', 'abysmal', 'accepted', 'accidentally', 'accompanied', 'accomplished', 'accomplishment', 'according', 'accounts', 'accurate', 'accuser', 'achieve', 'achieved', 'achievement', 'achievements', 'achieving', 'acknowledge', 'acknowledges', 'acquire', 'acquired', 'acted', 'acting', 'actions', 'activities', 'actors', 'acts', 'actually']\n"
     ]
    }
   ],
   "source": [
    "# Comparison of 3 vectorizers output\n",
    "\n",
    "features_raw = vectorizer.get_feature_names_out()\n",
    "features1 = vectorizer1.get_feature_names_out()\n",
    "features2 = vectorizer2.get_feature_names_out()\n",
    "\n",
    "print()\n",
    "print(\"specific to features1 (not in feature2):\")\n",
    "print(f'{[w for w in features1 if w not in features2]}')\n",
    "print()\n",
    "print(\"specific to features2 (not in feature1):\")\n",
    "print(f'{[w for w in features2 if w not in features1]}')\n",
    "\n",
    "print()\n",
    "print(\"specific to features_raw (not in feature2), selecting the first 100 features:\")\n",
    "print(f'{[w for w in features_raw if w not in features2][:100]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <u> Challenge questions <u>\n",
    "\n",
    "1. Normally, we simply need to use `.lower()` to perform casefolding, but Python also has a specific string method called `.casefold()`. What are the differences between these 2 methods, and in which situations do we need to use `.casefold()` over `.lower()`?\n",
    "\n",
    "2. Notice how the lemmatizer didn't fully return the root word of some words, like \"putting\", \"watered\", and \"cheapened\"? It's because the function wasn't called with the proper \"Part-Of-Speech\". Check out the difference by running the below code:\n",
    "    \n",
    "\n",
    "    print(lemmatizer.lemmatize(\"putting\")) # By default, WordNet infers this word as a Noun\n",
    "    print(lemmatizer.lemmatize(\"putting\", pos=\"v\"))\n",
    "\n",
    "By using the `pos` parameter, we force it to treat putting as a verb. Auto-detecting the part-of-speech of a word is actually another big NLP problem. The following site will give you more insights into how this is done: https://www.nltk.org/book/ch05.html. If you want a challenge, you can try to implement a lemmatizer that is enhanced with an auto part-of-speech tagging to increase the usefulness of the lemmatization (this is beyond the scope of the tutorial and subject, but a very satisfying problem to work on!)\n",
    "    \n",
    "3. An alternative vector representation for text data is TF-IDF (covered in the lecture). The `TfidfVectorizer` function works similarly to `CountVectorizer`. Have a look through its documentation here: https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html and produce a TF-IDF representation for the preprocessed corpus."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
